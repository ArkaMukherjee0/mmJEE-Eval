# mmJEE-Eval: A Multimodal Multilingual Dataset for Benchmarking VLMs



- mmJEE-Eval  is a multimodal and multilingual dataset for LLM evaluation comprising 1,460 challenging questions from seven years (2019-2025) of India's JEE Advanced competitive examination.
- Comprehensive evaluation of five state-of-the-art VLMs reveals that even frontier models struggle significantly: Gemini 2.5 Pro achieves 81.2% accuracy while OpenAI o3 reaches 77.9%, with smaller models performing substantially worse (â‰¤30%).
- mmJEE-Eval is significantly more challenging than the text-only JEEBench, the only other well-established dataset on JEE Advanced problems, with performance drops of 18-56% across all models.
- Our findings and manual error analysis demonstrate that contemporary VLMs remain limited in authentic scientific reasoning despite strong computational capabilities, establishing mmJEE-Eval~as a challenging benchmark that effectively discriminates between model capabilities.
