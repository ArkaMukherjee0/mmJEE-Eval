{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ee6099f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from collections import Counter, defaultdict\n",
    "import glob\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set publication-quality plot parameters\n",
    "plt.rcParams.update({\n",
    "    'font.size': 12,\n",
    "    'axes.titlesize': 14,\n",
    "    'axes.labelsize': 12,\n",
    "    'xtick.labelsize': 11,\n",
    "    'ytick.labelsize': 11,\n",
    "    'legend.fontsize': 11,\n",
    "    'figure.titlesize': 16,\n",
    "    'font.family': 'serif',\n",
    "    'font.serif': ['Times New Roman', 'DejaVu Serif', 'serif'],\n",
    "    'axes.linewidth': 1.2,\n",
    "    'grid.linewidth': 0.8,\n",
    "    'lines.linewidth': 2,\n",
    "    'patch.linewidth': 0.5,\n",
    "    'xtick.major.width': 1.2,\n",
    "    'ytick.major.width': 1.2,\n",
    "    'xtick.minor.width': 0.8,\n",
    "    'ytick.minor.width': 0.8,\n",
    "    'axes.spines.top': False,\n",
    "    'axes.spines.right': False,\n",
    "    'axes.grid': True,\n",
    "    #'axes.grid.alpha': 0.3,\n",
    "    'grid.alpha': 0.3\n",
    "})\n",
    "\n",
    "# Define a professional color palette\n",
    "COLORS = {\n",
    "    'primary': '#2E86AB',      # Professional blue\n",
    "    'secondary': '#A23B72',    # Deep magenta\n",
    "    'accent': '#F18F01',       # Warm orange\n",
    "    'success': '#C73E1D',      # Deep red\n",
    "    'neutral': '#6C757D',      # Gray\n",
    "    'light': '#E9ECEF',        # Light gray\n",
    "    'multimodal': '#FF6B6B',   # Coral red\n",
    "    'unimodal': '#4ECDC4',     # Teal\n",
    "    'gradient': ['#2E86AB', '#A23B72', '#F18F01', '#C73E1D', '#6C757D']\n",
    "}\n",
    "\n",
    "def load_all_evaluation_data(results_folder):\n",
    "    \"\"\"Load all JSON evaluation results from folder\"\"\"\n",
    "    json_files = glob.glob(os.path.join(results_folder, \"*.json\"))\n",
    "    \n",
    "    print(f\"Found {len(json_files)} result files in {results_folder}\")\n",
    "    \n",
    "    all_data = []\n",
    "    run_summaries = []\n",
    "    \n",
    "    for file_path in sorted(json_files):\n",
    "        try:\n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                data = json.load(f)\n",
    "                \n",
    "            # Extract run summary\n",
    "            run_summary = {\n",
    "                'run_id': data['run_id'],\n",
    "                'total_questions': data['total_questions'],\n",
    "                'correct_answers': data['correct_answers'],\n",
    "                'accuracy': data['accuracy'],\n",
    "                'duration': data['duration'],\n",
    "                'avg_time_per_question': data['avg_time_per_question'],\n",
    "                'timestamp': data['timestamp'],\n",
    "                'file_path': file_path\n",
    "            }\n",
    "            run_summaries.append(run_summary)\n",
    "            \n",
    "            # Add run metadata to each result\n",
    "            for result in data['results']:\n",
    "                result['run_timestamp'] = data['timestamp']\n",
    "                result['run_duration'] = data['duration']\n",
    "                result['run_accuracy'] = data['accuracy']\n",
    "                \n",
    "            all_data.extend(data['results'])\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error loading {file_path}: {e}\")\n",
    "    \n",
    "    return all_data, run_summaries\n",
    "\n",
    "def load_original_dataset(dataset_path):\n",
    "    \"\"\"Load the original JEE dataset to get requires_image field\"\"\"\n",
    "    try:\n",
    "        df_original = pd.read_csv(dataset_path)\n",
    "        # Create a mapping from question_id to requires_image\n",
    "        image_mapping = dict(zip(df_original['question_id'], df_original['requires_image']))\n",
    "        return image_mapping\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading original dataset {dataset_path}: {e}\")\n",
    "        return {}\n",
    "\n",
    "def load_multi_model_data(model_folders, dataset_path):\n",
    "    \"\"\"Load data from multiple model folders\"\"\"\n",
    "    all_models_data = {}\n",
    "    image_mapping = load_original_dataset(dataset_path)\n",
    "    \n",
    "    for model_name, folder_path in model_folders.items():\n",
    "        print(f\"\\nLoading data for {model_name}...\")\n",
    "        all_data, run_summaries = load_all_evaluation_data(folder_path)\n",
    "        \n",
    "        if all_data:\n",
    "            df = pd.DataFrame(all_data)\n",
    "            \n",
    "            # Add multimodal information from original dataset\n",
    "            df['requires_image'] = df['question_id'].map(image_mapping)\n",
    "            df['is_multimodal'] = df['requires_image'].fillna(False)\n",
    "            \n",
    "            # Add model name\n",
    "            df['model'] = model_name\n",
    "            \n",
    "            all_models_data[model_name] = {\n",
    "                'df': df,\n",
    "                'run_summaries': pd.DataFrame(run_summaries)\n",
    "            }\n",
    "            \n",
    "            print(f\"Loaded {len(df)} questions for {model_name}\")\n",
    "        else:\n",
    "            print(f\"No data found for {model_name}\")\n",
    "    \n",
    "    return all_models_data\n",
    "\n",
    "def create_accuracy_heatmap_by_model(all_models_data, save_path=None):\n",
    "    \"\"\"Create publication-quality accuracy heatmap for language performance only\"\"\"\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_df = pd.concat([data['df'] for data in all_models_data.values()], ignore_index=True)\n",
    "    \n",
    "    # Create pivot table for model vs language accuracy\n",
    "    model_language_accuracy = combined_df.groupby(['model', 'language'])['is_correct'].mean().reset_index()\n",
    "    pivot_model_language = model_language_accuracy.pivot(index='model', columns='language', values='is_correct')\n",
    "    \n",
    "    # Create the plot with larger fonts for publication quality\n",
    "    fig, ax = plt.subplots(figsize=(10, 8))\n",
    "    \n",
    "    # Custom colormap for better contrast\n",
    "    cmap = sns.diverging_palette(250, 10, n=256, as_cmap=True)\n",
    "    \n",
    "    # Model vs Language heatmap with larger fonts\n",
    "    im = sns.heatmap(pivot_model_language, \n",
    "                     annot=True, \n",
    "                     fmt='.3f',  # Show 3 decimal places for precision\n",
    "                     cmap=cmap,\n",
    "                     center=pivot_model_language.values.mean(),\n",
    "                     square=False,\n",
    "                     linewidths=1.0,  # Slightly thicker lines\n",
    "                     linecolor='white',\n",
    "                     cbar_kws={\n",
    "                         'label': 'Accuracy',\n",
    "                         'shrink': 0.8,\n",
    "                         'aspect': 15,\n",
    "                         'pad': 0.02\n",
    "                     },\n",
    "                     annot_kws={'fontsize': 16, 'fontweight': 'bold'},  # Larger annotation font\n",
    "                     ax=ax)\n",
    "    \n",
    "    # Remove title as requested\n",
    "    # ax.set_title('Model Performance by Language', fontweight='bold', pad=20)\n",
    "    \n",
    "    # Larger axis labels\n",
    "    ax.set_xlabel('Language', fontweight='bold', fontsize=18)\n",
    "    ax.set_ylabel('Model', fontweight='bold', fontsize=18)\n",
    "    \n",
    "    # Larger tick labels\n",
    "    ax.tick_params(axis='x', rotation=45, labelsize=16)\n",
    "    ax.tick_params(axis='y', rotation=0, labelsize=16)\n",
    "    \n",
    "    # Larger colorbar label\n",
    "    cbar = ax.collections[0].colorbar\n",
    "    cbar.set_label('Accuracy', fontweight='bold', fontsize=16)\n",
    "    cbar.ax.tick_params(labelsize=14)\n",
    "    \n",
    "    # Fine-tune layout\n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='pdf', bbox_inches='tight', dpi=300, \n",
    "                   facecolor='white', edgecolor='none')\n",
    "        print(f\"âœ“ Language accuracy heatmap saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    return pivot_model_language\n",
    "\n",
    "def create_multimodal_comparison(all_models_data, save_path=None):\n",
    "    \"\"\"Create publication-quality multimodal vs unimodal performance comparison - Bar chart only\"\"\"\n",
    "    \n",
    "    # Combine all data\n",
    "    combined_df = pd.concat([data['df'] for data in all_models_data.values()], ignore_index=True)\n",
    "    \n",
    "    # Filter out rows where requires_image is NaN\n",
    "    combined_df = combined_df.dropna(subset=['requires_image'])\n",
    "    \n",
    "    # Calculate performance by model and modality\n",
    "    multimodal_performance = combined_df.groupby(['model', 'is_multimodal'])['is_correct'].mean().reset_index()\n",
    "    \n",
    "    # Create pivot for plotting\n",
    "    pivot_multimodal = multimodal_performance.pivot(index='model', columns='is_multimodal', values='is_correct')\n",
    "    pivot_multimodal.columns = ['Text-Only', 'Multimodal']\n",
    "    \n",
    "    # Calculate sample sizes for annotations\n",
    "    sample_sizes = combined_df.groupby(['model', 'is_multimodal']).size().reset_index(name='count')\n",
    "    sample_pivot = sample_sizes.pivot(index='model', columns='is_multimodal', values='count')\n",
    "    sample_pivot.columns = ['Text-Only', 'Multimodal']\n",
    "    \n",
    "    # Create the publication-quality plot - Single bar chart only\n",
    "    fig, ax = plt.subplots(figsize=(12, 8))\n",
    "    \n",
    "    x = np.arange(len(pivot_multimodal.index))\n",
    "    width = 0.35\n",
    "    \n",
    "    bars1 = ax.bar(x - width/2, pivot_multimodal['Text-Only'], width, \n",
    "                   label='Image Optional Questions', color=COLORS['unimodal'], \n",
    "                   alpha=0.8, edgecolor='white', linewidth=1.5)\n",
    "    bars2 = ax.bar(x + width/2, pivot_multimodal['Multimodal'], width,\n",
    "                   label='Image Required Questions', color=COLORS['multimodal'], \n",
    "                   alpha=0.8, edgecolor='white', linewidth=1.5)\n",
    "    \n",
    "    # Add value labels on bars with sample sizes\n",
    "    def add_value_labels(bars, values, samples, offset=0.01):\n",
    "        for bar, val, n in zip(bars, values, samples):\n",
    "            height = bar.get_height()\n",
    "            ax.text(bar.get_x() + bar.get_width()/2., height + offset,\n",
    "                   f'{val:.3f}\\n(n={n})', ha='center', va='bottom', \n",
    "                   fontweight='bold', fontsize=10)\n",
    "    \n",
    "    add_value_labels(bars1, pivot_multimodal['Text-Only'], sample_pivot['Text-Only'])\n",
    "    add_value_labels(bars2, pivot_multimodal['Multimodal'], sample_pivot['Multimodal'])\n",
    "    \n",
    "    ax.set_xlabel('Model', fontweight='bold')\n",
    "    ax.set_ylabel('Accuracy', fontweight='bold')\n",
    "    # ax.set_title('Performance Comparison: Image Required vs Image Optional Questions', fontweight='bold', pad=20)\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(pivot_multimodal.index, rotation=0)\n",
    "    ax.legend(loc='upper right', frameon=True, fancybox=True, shadow=True)\n",
    "    ax.set_ylim(0, max(pivot_multimodal.max()) * 1.15)\n",
    "    ax.grid(axis='y', alpha=0.3, linestyle='--')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    \n",
    "    if save_path:\n",
    "        plt.savefig(save_path, format='pdf', bbox_inches='tight', dpi=300,\n",
    "                   facecolor='white', edgecolor='none')\n",
    "        print(f\"âœ“ Multimodal comparison saved to {save_path}\")\n",
    "    \n",
    "    plt.show()\n",
    "    \n",
    "    # Enhanced statistics output (keeping the same detailed analysis)\n",
    "    print(\"\\n\" + \"=\"*80)\n",
    "    print(\"ðŸ“Š MULTIMODAL VS TEXT-ONLY PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    # Create detailed table\n",
    "    detailed_results = pivot_multimodal.copy()\n",
    "    detailed_results['Difference (Multi - Text)'] = (detailed_results['Multimodal'] - \n",
    "                                                    detailed_results['Text-Only'])\n",
    "    detailed_results['Relative Change (%)'] = ((detailed_results['Difference (Multi - Text)'] / \n",
    "                                               detailed_results['Text-Only']) * 100)\n",
    "    \n",
    "    print(\"\\nDetailed Performance Metrics:\")\n",
    "    print(\"-\" * 80)\n",
    "    for col in detailed_results.columns:\n",
    "        if col in ['Text-Only', 'Multimodal']:\n",
    "            detailed_results[col] = detailed_results[col].apply(lambda x: f\"{x:.3f}\")\n",
    "        else:\n",
    "            detailed_results[col] = detailed_results[col].apply(lambda x: f\"{x:+.3f}\")\n",
    "    \n",
    "    print(detailed_results.to_string())\n",
    "    \n",
    "    # Summary statistics\n",
    "    print(f\"\\nðŸ“ˆ Summary Statistics:\")\n",
    "    print(f\"   â€¢ Average Text-Only Performance: {pivot_multimodal['Text-Only'].mean():.3f}\")\n",
    "    print(f\"   â€¢ Average Multimodal Performance: {pivot_multimodal['Multimodal'].mean():.3f}\")\n",
    "    print(f\"   â€¢ Overall Performance Gap: {(pivot_multimodal['Multimodal'] - pivot_multimodal['Text-Only']).mean():+.3f}\")\n",
    "    \n",
    "    better_multi = sum((pivot_multimodal['Multimodal'] - pivot_multimodal['Text-Only']) > 0)\n",
    "    total_models = len(pivot_multimodal)\n",
    "    print(f\"   â€¢ Models performing better on multimodal: {better_multi}/{total_models} ({better_multi/total_models*100:.1f}%)\")\n",
    "    \n",
    "    return pivot_multimodal\n",
    "\n",
    "def analyze_2025_vs_previous_years(all_models_data):\n",
    "    \"\"\"Analyze performance on 2025 questions vs average of previous years with enhanced formatting\"\"\"\n",
    "    \n",
    "    results_table = []\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*100)\n",
    "    print(\"ðŸ“… 2025 vs PREVIOUS YEARS PERFORMANCE COMPARISON\")\n",
    "    print(\"=\"*100)\n",
    "    \n",
    "    for model_name, data in all_models_data.items():\n",
    "        df = data['df']\n",
    "        \n",
    "        # Separate 2025 and previous years\n",
    "        df_2025 = df[df['year'] == 2025]\n",
    "        df_previous = df[df['year'] < 2025]\n",
    "        \n",
    "        if len(df_2025) > 0 and len(df_previous) > 0:\n",
    "            # Calculate accuracies\n",
    "            accuracy_2025 = df_2025['is_correct'].mean()\n",
    "            accuracy_previous = df_previous['is_correct'].mean()\n",
    "            \n",
    "            # Count questions\n",
    "            count_2025 = len(df_2025)\n",
    "            count_previous = len(df_previous)\n",
    "            \n",
    "            # Calculate difference\n",
    "            difference = accuracy_2025 - accuracy_previous\n",
    "            \n",
    "            # Get year range for previous years\n",
    "            previous_years = sorted(df_previous['year'].unique())\n",
    "            year_range = f\"{min(previous_years)}-{max(previous_years)}\"\n",
    "            \n",
    "            # Calculate statistical significance (basic test)\n",
    "            from scipy.stats import chi2_contingency\n",
    "            \n",
    "            # Create contingency table\n",
    "            contingency = np.array([\n",
    "                [df_previous['is_correct'].sum(), len(df_previous) - df_previous['is_correct'].sum()],\n",
    "                [df_2025['is_correct'].sum(), len(df_2025) - df_2025['is_correct'].sum()]\n",
    "            ])\n",
    "            \n",
    "            try:\n",
    "                chi2, p_value, _, _ = chi2_contingency(contingency)\n",
    "                significance = \"***\" if p_value < 0.001 else \"**\" if p_value < 0.01 else \"*\" if p_value < 0.05 else \"\"\n",
    "            except:\n",
    "                significance = \"\"\n",
    "                p_value = np.nan\n",
    "            \n",
    "            results_table.append({\n",
    "                'Model': model_name,\n",
    "                'Years (Previous)': year_range,\n",
    "                'Previous Accuracy': f\"{accuracy_previous:.3f}\",\n",
    "                'Previous Count': count_previous,\n",
    "                '2025 Accuracy': f\"{accuracy_2025:.3f}\",\n",
    "                '2025 Count': count_2025,\n",
    "                'Difference': f\"{difference:+.3f}{significance}\",\n",
    "                'Relative Change (%)': f\"{(difference / accuracy_previous) * 100:+.1f}%\",\n",
    "                'p-value': f\"{p_value:.4f}\" if not np.isnan(p_value) else \"N/A\"\n",
    "            })\n",
    "            \n",
    "        else:\n",
    "            print(f\"âš ï¸  Warning: {model_name} doesn't have data for both 2025 and previous years\")\n",
    "    \n",
    "    # Create and display enhanced DataFrame\n",
    "    if results_table:\n",
    "        results_df = pd.DataFrame(results_table)\n",
    "        \n",
    "        print(\"\\nðŸ“Š Comprehensive Performance Comparison:\")\n",
    "        print(\"-\" * 100)\n",
    "        \n",
    "        # Create a formatted table string\n",
    "        col_widths = {\n",
    "            'Model': 12,\n",
    "            'Years (Previous)': 10,\n",
    "            'Previous Accuracy': 12,\n",
    "            'Previous Count': 8,\n",
    "            '2025 Accuracy': 12,\n",
    "            '2025 Count': 8,\n",
    "            'Difference': 12,\n",
    "            'Relative Change (%)': 15,\n",
    "            'p-value': 8\n",
    "        }\n",
    "        \n",
    "        # Print header\n",
    "        header = \"|\".join([col.center(col_widths[col]) for col in results_df.columns])\n",
    "        print(header)\n",
    "        print(\"-\" * len(header))\n",
    "        \n",
    "        # Print rows\n",
    "        for _, row in results_df.iterrows():\n",
    "            formatted_row = \"|\".join([\n",
    "                str(row[col]).center(col_widths[col]) for col in results_df.columns\n",
    "            ])\n",
    "            print(formatted_row)\n",
    "        \n",
    "        print(\"-\" * len(header))\n",
    "        \n",
    "        # Enhanced summary statistics\n",
    "        print(f\"\\nðŸ“ˆ Key Insights:\")\n",
    "        \n",
    "        # Convert back to numeric for calculations\n",
    "        numeric_diffs = [float(row['Difference'].replace('*', '').replace('+', '')) \n",
    "                        for _, row in results_df.iterrows()]\n",
    "        numeric_changes = [float(row['Relative Change (%)'].replace('%', '').replace('+', '')) \n",
    "                          for _, row in results_df.iterrows()]\n",
    "        \n",
    "        avg_diff = np.mean(numeric_diffs)\n",
    "        avg_change = np.mean(numeric_changes)\n",
    "        \n",
    "        better_count = sum(1 for diff in numeric_diffs if diff > 0)\n",
    "        worse_count = sum(1 for diff in numeric_diffs if diff < 0)\n",
    "        total_count = len(numeric_diffs)\n",
    "        \n",
    "        print(f\"   â€¢ Average performance difference: {avg_diff:+.3f}\")\n",
    "        print(f\"   â€¢ Average relative change: {avg_change:+.1f}%\")\n",
    "        print(f\"   â€¢ Models performing better on 2025: {better_count}/{total_count} ({better_count/total_count*100:.1f}%)\")\n",
    "        print(f\"   â€¢ Models performing worse on 2025: {worse_count}/{total_count} ({worse_count/total_count*100:.1f}%)\")\n",
    "        \n",
    "        if any('*' in str(row['Difference']) for _, row in results_df.iterrows()):\n",
    "            print(f\"\\nðŸ“‹ Statistical Significance: * p<0.05, ** p<0.01, *** p<0.001\")\n",
    "        \n",
    "        return pd.DataFrame([{\n",
    "            'Model': row['Model'],\n",
    "            'Previous_Years_Range': row['Years (Previous)'],\n",
    "            'Previous_Years_Accuracy': float(row['Previous Accuracy']),\n",
    "            'Previous_Years_Count': row['Previous Count'],\n",
    "            '2025_Accuracy': float(row['2025 Accuracy']),\n",
    "            '2025_Count': row['2025 Count'],\n",
    "            'Difference': float(row['Difference'].replace('*', '').replace('+', '')),\n",
    "            'Relative_Change_Percent': float(row['Relative Change (%)'].replace('%', '').replace('+', '')),\n",
    "            'p_value': float(row['p-value']) if row['p-value'] != 'N/A' else np.nan\n",
    "        } for _, row in results_df.iterrows()])\n",
    "    \n",
    "    return pd.DataFrame()\n",
    "\n",
    "def run_multi_model_analysis(model_folders, dataset_path, save_plots=True):\n",
    "    \"\"\"Run the complete multi-model analysis with enhanced output\"\"\"\n",
    "    \n",
    "    print(\"ðŸš€ \" + \"=\"*80)\n",
    "    print(\"   MULTI-MODEL PERFORMANCE ANALYSIS PIPELINE\")\n",
    "    print(\"=\"*85)\n",
    "    print(f\"ðŸ“ Models to analyze: {', '.join(model_folders.keys())}\")\n",
    "    print(f\"ðŸ“Š Original dataset: {os.path.basename(dataset_path)}\")\n",
    "    print(f\"ðŸ’¾ Save plots as PDF: {'Yes' if save_plots else 'No'}\")\n",
    "    print(\"-\"*85)\n",
    "    \n",
    "    # Load data from all models\n",
    "    print(\"\\nðŸ“¥ Loading evaluation data...\")\n",
    "    all_models_data = load_multi_model_data(model_folders, dataset_path)\n",
    "    \n",
    "    if not all_models_data:\n",
    "        print(\"âŒ No data loaded. Please check your folder paths.\")\n",
    "        return\n",
    "    \n",
    "    # Print data summary\n",
    "    print(f\"\\nâœ… Successfully loaded data for {len(all_models_data)} models:\")\n",
    "    total_questions = 0\n",
    "    for model_name, data in all_models_data.items():\n",
    "        n_questions = len(data['df'])\n",
    "        total_questions += n_questions\n",
    "        print(f\"   â€¢ {model_name}: {n_questions:,} questions\")\n",
    "    print(f\"   ðŸ“Š Total questions across all models: {total_questions:,}\")\n",
    "    \n",
    "    # 1. Create accuracy heatmap by model\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ“ˆ 1. GENERATING ACCURACY HEATMAPS\")\n",
    "    print(\"=\"*50)\n",
    "    heatmap_path = \"model_accuracy_heatmap.pdf\" if save_plots else None\n",
    "    pivot_subject, pivot_language = create_accuracy_heatmap_by_model(all_models_data, heatmap_path)\n",
    "    \n",
    "    # 2. Create multimodal vs unimodal comparison\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ–¼ï¸  2. ANALYZING MULTIMODAL PERFORMANCE\")\n",
    "    print(\"=\"*50)\n",
    "    multimodal_path = \"multimodal_comparison.pdf\" if save_plots else None\n",
    "    multimodal_results = create_multimodal_comparison(all_models_data, multimodal_path)\n",
    "    \n",
    "    # 3. Analyze 2025 vs previous years\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"ðŸ“… 3. TEMPORAL PERFORMANCE ANALYSIS\")\n",
    "    print(\"=\"*50)\n",
    "    year_comparison = analyze_2025_vs_previous_years(all_models_data)\n",
    "    \n",
    "    # Final summary\n",
    "    print(\"\\n\" + \"ðŸŽ¯ \" + \"=\"*80)\n",
    "    print(\"   ANALYSIS COMPLETE - SUMMARY\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    if save_plots:\n",
    "        print(\"ðŸ“„ Generated Files:\")\n",
    "        print(\"   â€¢ model_accuracy_heatmap.pdf - Subject & Language performance heatmaps\")\n",
    "        print(\"   â€¢ multimodal_comparison.pdf - Multimodal vs Text-only comparison\")\n",
    "    \n",
    "    print(\"\\nðŸ“Š Key Findings:\")\n",
    "    \n",
    "    # Overall best performing model\n",
    "    combined_df = pd.concat([data['df'] for data in all_models_data.values()], ignore_index=True)\n",
    "    overall_performance = combined_df.groupby('model')['is_correct'].mean().sort_values(ascending=False)\n",
    "    best_model = overall_performance.index[0]\n",
    "    best_accuracy = overall_performance.iloc[0]\n",
    "    \n",
    "    print(f\"   ðŸ† Best overall model: {best_model} ({best_accuracy:.3f} accuracy)\")\n",
    "    \n",
    "    # Multimodal performance leader\n",
    "    if 'Multimodal' in multimodal_results.columns:\n",
    "        multi_leader = multimodal_results['Multimodal'].idxmax()\n",
    "        multi_score = multimodal_results.loc[multi_leader, 'Multimodal']\n",
    "        print(f\"   ðŸ–¼ï¸  Best multimodal performance: {multi_leader} ({multi_score:.3f} accuracy)\")\n",
    "    \n",
    "    # 2025 performance\n",
    "    if not year_comparison.empty and '2025_Accuracy' in year_comparison.columns:\n",
    "        best_2025 = year_comparison.loc[year_comparison['2025_Accuracy'].idxmax(), 'Model']\n",
    "        best_2025_score = year_comparison['2025_Accuracy'].max()\n",
    "        print(f\"   ðŸ“… Best 2025 performance: {best_2025} ({best_2025_score:.3f} accuracy)\")\n",
    "    \n",
    "    print(\"\\nâœ¨ Analysis pipeline completed successfully!\")\n",
    "    print(\"=\"*85)\n",
    "    \n",
    "    return {\n",
    "        'model_data': all_models_data,\n",
    "        'accuracy_by_subject': pivot_subject,\n",
    "        'accuracy_by_language': pivot_language,\n",
    "        'multimodal_comparison': multimodal_results,\n",
    "        'year_comparison': year_comparison,\n",
    "        'overall_performance': overall_performance\n",
    "    }\n",
    "\n",
    "\n",
    "model_folders = {\n",
    "    \"Gemma 3  27B\": \"gemma3_evaluation_results\",\n",
    "    \"Gemini 2.5 Pro\": \"gemini25_evaluation_results\",\n",
    "    \"InternVL3 8B\": \"internvl3_8b_evaluation_results\",\n",
    "    \"Qwen 2.5 VL 7B\": \"qwen25_evaluation_results\"\n",
    "}\n",
    "\n",
    "# Path to the original dataset\n",
    "dataset_path = \"jee_advanced_combined_fixed.csv\"\n",
    "\n",
    "# Run the analysis with publication-quality outputs\n",
    "print(\"ðŸŽ¯ Starting publication-ready analysis...\")\n",
    "results = run_multi_model_analysis(model_folders, dataset_path, save_plots=True)\n",
    "\n",
    "if results:\n",
    "    print(\"\\nðŸŽ‰ Success!\")\n",
    "else:\n",
    "    print(\"âŒ Analysis failed. Please check your file paths and data.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
