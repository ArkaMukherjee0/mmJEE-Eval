{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d4ee5e96",
   "metadata": {},
   "source": [
    "## Reasoning trace experiments:\n",
    "- Pick 100 wrong questions\n",
    "\n",
    "- Questions to study:\n",
    "    1. Conceptual errors: Did the VLM make any conceptual errors? (Inability to use the correct concepts required for a certain question)\n",
    "    2. Grounding errors: Did the VLM model the question appropriately? (Utilizing the correct formulas based on the concepts, appropriate mathematical modeling, correct equation setups, etc.)\n",
    "    3. Computation errors: Is the algebraic manipulation and arithmetic correct?\n",
    "    4. Instruction FOllowing errors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52200df8",
   "metadata": {},
   "source": [
    "## Selecting 100 wrong responses at random from OpenAI o3, InternVL3 8B, and Gemini 2.5 Pro"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1f8068a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import random\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "def load_json_file(file_path):\n",
    "    \"\"\"Load JSON file and return the results list\"\"\"\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as f:\n",
    "            data = json.load(f)\n",
    "        return data.get('results', [])\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading {file_path}: {e}\")\n",
    "        return []\n",
    "\n",
    "def filter_incorrect_responses(results, model_name):\n",
    "    \"\"\"Filter to only include incorrect responses (is_correct = False)\"\"\"\n",
    "    incorrect = []\n",
    "    for result in results:\n",
    "        if result.get('is_correct', True) == False:  # Only wrong answers\n",
    "            incorrect.append(result)\n",
    "    \n",
    "    print(f\"{model_name}: {len(results)} total responses, {len(incorrect)} incorrect responses\")\n",
    "    return incorrect\n",
    "\n",
    "def filter_o3_responses(results):\n",
    "    \"\"\"Filter O3 responses to only include those >70 characters\"\"\"\n",
    "    filtered = []\n",
    "    for result in results:\n",
    "        full_response = result.get('full_response', '')\n",
    "        if len(full_response) > 70:\n",
    "            filtered.append(result)\n",
    "    \n",
    "    print(f\"O3: After length filter: {len(filtered)} responses >70 chars\")\n",
    "    return filtered\n",
    "\n",
    "def select_random_questions(file_paths, questions_per_model=33):\n",
    "    \"\"\"\n",
    "    Select random questions from each model\n",
    "    \n",
    "    Args:\n",
    "        file_paths: Dictionary with model names as keys and file paths as values\n",
    "        questions_per_model: Number of questions to select per model (default: 33)\n",
    "    \"\"\"\n",
    "    \n",
    "    all_selected = []\n",
    "    selection_summary = {}\n",
    "    \n",
    "    for model_name, file_path in file_paths.items():\n",
    "        print(f\"\\nProcessing {model_name}...\")\n",
    "        \n",
    "        # Load results\n",
    "        results = load_json_file(file_path)\n",
    "        \n",
    "        if not results:\n",
    "            print(f\"No results found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Filter to only incorrect responses first\n",
    "        results = filter_incorrect_responses(results, model_name)\n",
    "        \n",
    "        if not results:\n",
    "            print(f\"No incorrect responses found for {model_name}\")\n",
    "            continue\n",
    "        \n",
    "        # Apply O3 length filter if needed (after incorrect filter)\n",
    "        if model_name.lower() == 'o3':\n",
    "            results = filter_o3_responses(results)\n",
    "        \n",
    "        # Random selection\n",
    "        if len(results) < questions_per_model:\n",
    "            print(f\"Warning: {model_name} has only {len(results)} questions, selecting all\")\n",
    "            selected = results\n",
    "        else:\n",
    "            selected = random.sample(results, questions_per_model)\n",
    "        \n",
    "        # Add model identifier to each selected question\n",
    "        for question in selected:\n",
    "            question['model_name'] = model_name\n",
    "            question['source_file'] = file_path\n",
    "        \n",
    "        all_selected.extend(selected)\n",
    "        selection_summary[model_name] = {\n",
    "            'total_available': len(load_json_file(file_path)),  # Original total\n",
    "            'incorrect_available': len(results),  # After filtering for incorrect\n",
    "            'selected': len(selected),\n",
    "            'file_path': file_path\n",
    "        }\n",
    "        \n",
    "        print(f\"Selected {len(selected)} questions from {model_name}\")\n",
    "    \n",
    "    return all_selected, selection_summary\n",
    "\n",
    "def save_selected_questions(selected_questions, output_file='selected_incorrect_questions_for_manual_review.json'):\n",
    "    \"\"\"Save selected questions to JSON file\"\"\"\n",
    "    \n",
    "    # Create summary data\n",
    "    summary_data = {\n",
    "        'selection_info': {\n",
    "            'total_selected': len(selected_questions),\n",
    "            'selection_date': pd.Timestamp.now().strftime('%Y-%m-%d %H:%M:%S'),\n",
    "            'selection_method': 'random_sampling_incorrect_only',\n",
    "            'filter_criteria': 'is_correct == False'\n",
    "        },\n",
    "        'selected_questions': selected_questions\n",
    "    }\n",
    "    \n",
    "    # Save to file\n",
    "    with open(output_file, 'w', encoding='utf-8') as f:\n",
    "        json.dump(summary_data, f, indent=2, ensure_ascii=False)\n",
    "    \n",
    "    print(f\"\\nSelected questions saved to: {output_file}\")\n",
    "    return output_file\n",
    "\n",
    "def create_summary_report(selection_summary, selected_questions):\n",
    "    \"\"\"Create a summary report of the selection\"\"\"\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"SELECTION SUMMARY REPORT\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # Overall summary\n",
    "    total_selected = len(selected_questions)\n",
    "    print(f\"Total questions selected: {total_selected}\")\n",
    "    \n",
    "    # Per-model breakdown\n",
    "    print(f\"\\nPer-model breakdown:\")\n",
    "    for model, info in selection_summary.items():\n",
    "        print(f\"  {model}:\")\n",
    "        print(f\"    - Total responses: {info['total_available']}\")\n",
    "        print(f\"    - Incorrect responses: {info['incorrect_available']}\")\n",
    "        print(f\"    - Selected for review: {info['selected']}\")\n",
    "        print(f\"    - File: {Path(info['file_path']).name}\")\n",
    "    \n",
    "    # Question type distribution\n",
    "    print(f\"\\nQuestion type distribution in selected set:\")\n",
    "    question_types = {}\n",
    "    subjects = {}\n",
    "    languages = {}\n",
    "    \n",
    "    for q in selected_questions:\n",
    "        # Question type\n",
    "        qtype = q.get('question_type', 'Unknown')\n",
    "        question_types[qtype] = question_types.get(qtype, 0) + 1\n",
    "        \n",
    "        # Subject\n",
    "        subject = q.get('subject', 'Unknown')\n",
    "        subjects[subject] = subjects.get(subject, 0) + 1\n",
    "        \n",
    "        # Language\n",
    "        language = q.get('language', 'Unknown')\n",
    "        languages[language] = languages.get(language, 0) + 1\n",
    "    \n",
    "    print(f\"  Question Types: {dict(sorted(question_types.items()))}\")\n",
    "    print(f\"  Subjects: {dict(sorted(subjects.items()))}\")\n",
    "    print(f\"  Languages: {dict(sorted(languages.items()))}\")\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "random.seed(42)\n",
    "\n",
    "# Define file paths\n",
    "file_paths = {\n",
    "    'gemini25': r'gemini25_evaluation_results\\gemini25_run_04_20250724_202554.json',\n",
    "    'o3': r'o3_evaluation_results\\o3_run_01_partial_20250719_092641.json',\n",
    "    'internvl3_8b': r'internvl3_8b_evaluation_results\\internvl3_8b_run_06_20250725_201612.json'\n",
    "}\n",
    "\n",
    "questions_per_model = 33  # 33 + 33 + 33 = 99\n",
    "\n",
    "print(\"Starting random question selection from INCORRECT responses only...\")\n",
    "print(f\"Target: ~{questions_per_model} incorrect questions per model\")\n",
    "\n",
    "# Select questions\n",
    "selected_questions, selection_summary = select_random_questions(\n",
    "    file_paths, questions_per_model\n",
    ")\n",
    "\n",
    "# Adjust if we need exactly 100\n",
    "if len(selected_questions) > 100:\n",
    "    selected_questions = random.sample(selected_questions, 100)\n",
    "    print(f\"\\nAdjusted to exactly 100 questions\")\n",
    "\n",
    "# Save results\n",
    "output_file = save_selected_questions(selected_questions)\n",
    "\n",
    "# Create summary report\n",
    "create_summary_report(selection_summary, selected_questions)\n",
    "\n",
    "print(f\"\\n\" + \"=\"*60)\n",
    "print(\"SELECTION COMPLETE!\")\n",
    "print(f\"Output file: {output_file}\")\n",
    "print(\"=\"*60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b67d6bba",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
