{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb7720",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1. First, test if the model is accessible:\\ntest_model_connection()\\n\\n# 2. To inspect the dataset first:\\ndf = load_jeebench_sample(10)\\n\\n# 3. To start new evaluation:\\nrun_evaluation()\\n\\n# 4. To resume from where you left off:\\nresume_evaluation()\\n\\n# 5. To check current progress:\\ncheck_progress()\\n\\n# 6. If you get state compatibility errors, try:\\nforce_upgrade_state()\\n\\n# 7. To recover from partial results if state is corrupted:\\nrecover_from_partial_results()\\n\\n# 8. To reset everything (use carefully!):\\nreset_evaluation()\\n\\n# Example of running a single question for testing:\\ndef test_single_question():\\n    evaluator = JEEBenchQwen25VLEvaluator(1, MODEL_NAME)\\n    sample_question = evaluator.df.iloc[0]\\n    result = evaluator.evaluate_single_question(sample_question, 1, 0)\\n    print(\"Test result:\", result)\\n    return result\\n\\n# Note: With the new implementation, state is saved every 5 questions\\n# and partial results are saved every 25 questions, so you\\'ll lose at most\\n# 4 questions of work if the evaluation is interrupted.\\n\\n# If you encounter \\'EvaluationState\\' object has no attribute errors:\\n# This happens when you have an old state file. Try force_upgrade_state()\\n# or reset_evaluation() to start fresh.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import signal\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "from datasets import load_dataset\n",
    "import lmstudio as lms\n",
    "\n",
    "# Configure logging with UTF-8 encoding to handle Unicode characters\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('jeebench_qwen2_5vl_evaluation.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class EvaluationState:\n",
    "    \"\"\"Persistent state for resuming evaluation\"\"\"\n",
    "    current_run: int\n",
    "    current_run_question_idx: int  # Which question within the current run\n",
    "    completed_questions: int\n",
    "    total_questions: int\n",
    "    all_run_summaries: List[Dict]\n",
    "    failed_questions: List[Dict]\n",
    "    current_run_results: List[Dict]  # Results for the current incomplete run\n",
    "    current_run_shuffled_indices: List[int]  # Shuffled indices for current run\n",
    "    start_time: float\n",
    "    last_save_time: float\n",
    "\n",
    "class Qwen25VLClient:\n",
    "    \"\"\"Local Qwen 2.5 VL 7B client via LM Studio API\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"qwen/qwen2.5-vl-7b\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        try:\n",
    "            # Initialize LM Studio model\n",
    "            self.model = lms.llm(model_name)\n",
    "            logger.info(f\"‚úÖ Qwen 2.5 VL 7B model loaded successfully: {model_name}\")\n",
    "            \n",
    "            # Test the model with a simple query\n",
    "            test_response = self.model.respond(\"Hello, can you see this?\")\n",
    "            test_response = test_response.content\n",
    "            logger.info(f\"üìù Model test response: {test_response[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Failed to initialize Qwen 2.5 VL model: {e}\")\n",
    "            raise ValueError(f\"Could not load Qwen 2.5 VL model '{model_name}'. Please ensure LM Studio is running and the model is available.\")\n",
    "    \n",
    "    def generate_content(self, prompt: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"Generate content using Qwen 2.5 VL model with retry logic\"\"\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.debug(f\"üîÑ Generating content (attempt {attempt + 1}/{max_retries})\")\n",
    "                \n",
    "                # Use LM Studio API to get response\n",
    "                response = self.model.respond(prompt)\n",
    "                \n",
    "                if response and response.content:\n",
    "                    return response.content\n",
    "                else:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Empty response on attempt {attempt + 1}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(f\"üí• All {max_retries} attempts failed\")\n",
    "                    return None\n",
    "                \n",
    "                # Wait before retry\n",
    "                time.sleep(2 ** attempt)\n",
    "        \n",
    "        return None\n",
    "\n",
    "class JEEBenchQwen25VLEvaluator:\n",
    "    def __init__(self, num_runs: int = 10, model_name: str = \"qwen/qwen2.5-vl-7b\"):\n",
    "        self.num_runs = num_runs\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Load JEEBench dataset\n",
    "        logger.info(\"üìö Loading JEEBench dataset...\")\n",
    "        self.dataset = load_dataset(\"daman1209arora/jeebench\")\n",
    "        \n",
    "        # Convert to pandas DataFrame for easier manipulation\n",
    "        self.df = pd.DataFrame(self.dataset['test'])\n",
    "        logger.info(f\"üìä Loaded JEEBench dataset with {len(self.df)} questions\")\n",
    "        \n",
    "        # Print dataset info\n",
    "        logger.info(f\"üìã Dataset columns: {list(self.df.columns)}\")\n",
    "        logger.info(f\"üìù Question types: {self.df['type'].value_counts().to_dict()}\")\n",
    "        logger.info(f\"üìö Subjects: {self.df['subject'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Initialize Qwen 2.5 VL client\n",
    "        logger.info(\"üöÄ Initializing Qwen 2.5 VL 7B model...\")\n",
    "        self.client = Qwen25VLClient(model_name)\n",
    "        \n",
    "        # Results and state management\n",
    "        self.results_dir = Path(\"jeebench_qwen25vl_evaluation_results\")\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for organization\n",
    "        (self.results_dir / \"partial_results\").mkdir(exist_ok=True)\n",
    "        (self.results_dir / \"completed_runs\").mkdir(exist_ok=True)\n",
    "        (self.results_dir / \"state_backups\").mkdir(exist_ok=True)\n",
    "        \n",
    "        self.state_file = self.results_dir / \"evaluation_state.pkl\"\n",
    "        self.state = self.load_or_create_state()\n",
    "        \n",
    "        # Control flags\n",
    "        self.stop_requested = False\n",
    "        self.interrupted = False\n",
    "        \n",
    "        logger.info(f\"üíæ Results will be saved to: {self.results_dir}\")\n",
    "        logger.info(f\"üéØ Running {num_runs} evaluations with Qwen 2.5 VL 7B\")\n",
    "    \n",
    "    def load_or_create_state(self) -> EvaluationState:\n",
    "        \"\"\"Load existing state or create new one with enhanced recovery\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            try:\n",
    "                with open(self.state_file, 'rb') as f:\n",
    "                    state = pickle.load(f)\n",
    "                \n",
    "                # Handle backward compatibility for old state files\n",
    "                # Check if this is an old state format and upgrade it\n",
    "                if not hasattr(state, 'current_run_question_idx'):\n",
    "                    logger.info(\"üîÑ Upgrading old state format...\")\n",
    "                    state.current_run_question_idx = 0\n",
    "                if not hasattr(state, 'current_run_results'):\n",
    "                    state.current_run_results = []\n",
    "                if not hasattr(state, 'current_run_shuffled_indices'):\n",
    "                    state.current_run_shuffled_indices = []\n",
    "                \n",
    "                # Save the upgraded state immediately\n",
    "                self.state = state\n",
    "                self.save_state()\n",
    "                \n",
    "                logger.info(f\"üîÑ Resumed from Run {state.current_run}, Question {state.current_run_question_idx + 1} within run, Total: {state.completed_questions}/{state.total_questions}\")\n",
    "                return state\n",
    "            except Exception as e:\n",
    "                logger.error(f\"‚ùå Error loading state: {e}\")\n",
    "                \n",
    "                # Try to load from backup\n",
    "                backup_file = self.state_file.with_suffix('.backup')\n",
    "                if backup_file.exists():\n",
    "                    try:\n",
    "                        with open(backup_file, 'rb') as f:\n",
    "                            state = pickle.load(f)\n",
    "                        \n",
    "                        # Handle backward compatibility for old backup files\n",
    "                        if not hasattr(state, 'current_run_question_idx'):\n",
    "                            logger.info(\"üîÑ Upgrading old backup state format...\")\n",
    "                            state.current_run_question_idx = 0\n",
    "                        if not hasattr(state, 'current_run_results'):\n",
    "                            state.current_run_results = []\n",
    "                        if not hasattr(state, 'current_run_shuffled_indices'):\n",
    "                            state.current_run_shuffled_indices = []\n",
    "                        \n",
    "                        # Save the upgraded state\n",
    "                        self.state = state\n",
    "                        self.save_state()\n",
    "                            \n",
    "                        logger.info(f\"üîÑ Recovered from backup: Run {state.current_run}, Question {state.current_run_question_idx + 1} within run, Total: {state.completed_questions}/{state.total_questions}\")\n",
    "                        return state\n",
    "                    except Exception as backup_error:\n",
    "                        logger.error(f\"‚ùå Error loading backup state: {backup_error}\")\n",
    "        \n",
    "        # Create new state\n",
    "        logger.info(\"üÜï Creating new evaluation state\")\n",
    "        return EvaluationState(\n",
    "            current_run=1,\n",
    "            current_run_question_idx=0,\n",
    "            completed_questions=0,\n",
    "            total_questions=len(self.df) * self.num_runs,\n",
    "            all_run_summaries=[],\n",
    "            failed_questions=[],\n",
    "            current_run_results=[],\n",
    "            current_run_shuffled_indices=[],\n",
    "            start_time=time.time(),\n",
    "            last_save_time=time.time()\n",
    "        )\n",
    "    \n",
    "    def _convert_to_json_serializable(self, obj):\n",
    "        \"\"\"Convert numpy/pandas types to JSON serializable types\"\"\"\n",
    "        if hasattr(obj, 'item'):  # numpy scalar\n",
    "            return obj.item()\n",
    "        elif hasattr(obj, 'tolist'):  # numpy array\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_to_json_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def _save_partial_run_results(self, run_id: int, results: List[Dict], questions_completed: int):\n",
    "        \"\"\"Save partial results during a run to prevent data loss\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"jeebench_qwen25vl_run_{run_id:02d}_partial_{questions_completed}q_{timestamp}.json\"\n",
    "        filepath = self.results_dir / \"partial_results\" / filename\n",
    "        \n",
    "        # Create partial results directory if it doesn't exist\n",
    "        filepath.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Calculate current accuracy for this partial run\n",
    "        correct_count = sum(1 for r in results if r['is_correct'])\n",
    "        accuracy = (correct_count / len(results)) * 100 if results else 0\n",
    "        \n",
    "        partial_summary = {\n",
    "            'run_id': int(run_id),\n",
    "            'model_name': str(self.model_name),\n",
    "            'questions_completed': int(questions_completed),\n",
    "            'total_questions_in_run': int(len(self.df)),\n",
    "            'partial_results_count': int(len(results)),\n",
    "            'correct_answers': int(correct_count),\n",
    "            'partial_accuracy': float(accuracy),\n",
    "            'timestamp': str(timestamp),\n",
    "            'is_partial': True,\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Convert to JSON serializable format\n",
    "            serializable_summary = self._convert_to_json_serializable(partial_summary)\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_summary, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"üíæ Partial results saved: {questions_completed} questions, {accuracy:.1f}% accuracy\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error saving partial results: {e}\")\n",
    "            # Try to save a simplified version without full results\n",
    "            try:\n",
    "                simple_summary = {\n",
    "                    'run_id': int(run_id),\n",
    "                    'questions_completed': int(questions_completed),\n",
    "                    'correct_answers': int(correct_count),\n",
    "                    'partial_accuracy': float(accuracy),\n",
    "                    'timestamp': str(timestamp),\n",
    "                    'error': 'Full results could not be serialized'\n",
    "                }\n",
    "                simple_filename = f\"jeebench_qwen25vl_run_{run_id:02d}_partial_simple_{questions_completed}q_{timestamp}.json\"\n",
    "                simple_filepath = filepath.parent / simple_filename\n",
    "                with open(simple_filepath, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(simple_summary, f, indent=2, ensure_ascii=False)\n",
    "                logger.info(f\"üíæ Simplified partial results saved as fallback\")\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(f\"üí• Even simplified save failed: {fallback_error}\")\n",
    "\n",
    "    def save_state(self):\n",
    "        \"\"\"Save current evaluation state with enhanced error handling\"\"\"\n",
    "        self.state.last_save_time = time.time()\n",
    "        try:\n",
    "            # Use temporary file for atomic write\n",
    "            temp_file = self.state_file.with_suffix('.tmp')\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                pickle.dump(self.state, f)\n",
    "            temp_file.replace(self.state_file)\n",
    "            \n",
    "            # Also create a backup of the state file\n",
    "            backup_file = self.state_file.with_suffix('.backup')\n",
    "            with open(backup_file, 'wb') as f:\n",
    "                pickle.dump(self.state, f)\n",
    "            \n",
    "            logger.debug(f\"üíæ State saved: Run {self.state.current_run}, Question {self.state.current_run_question_idx + 1} within run, Total: {self.state.completed_questions}/{self.state.total_questions}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error saving state: {e}\")\n",
    "            # Try to save to backup location\n",
    "            try:\n",
    "                emergency_backup = self.results_dir / f\"emergency_state_backup_{int(time.time())}.pkl\"\n",
    "                with open(emergency_backup, 'wb') as f:\n",
    "                    pickle.dump(self.state, f)\n",
    "                logger.info(f\"üö® Emergency backup saved to: {emergency_backup}\")\n",
    "            except Exception as backup_error:\n",
    "                logger.error(f\"üí• Failed to create emergency backup: {backup_error}\")\n",
    "    \n",
    "    def create_question_prompt(self, question_data: pd.Series) -> str:\n",
    "        \"\"\"Create appropriate prompt based on question type\"\"\"\n",
    "        question_type = question_data['type']\n",
    "        question_text = question_data['question']\n",
    "        \n",
    "        # Base prompt with question\n",
    "        prompt = f\"You are an expert at solving JEE (Joint Entrance Examination) problems. Please solve this question step by step.\\n\\nQuestion: {question_text}\\n\\n\"\n",
    "        \n",
    "        # Add type-specific instructions\n",
    "        if question_type == \"MCQ\":\n",
    "            prompt += \"\"\"This is a multiple choice question. Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "\n",
    "For this question:\n",
    "- Choose exactly ONE option (A, B, C, or D)\n",
    "- Show your reasoning clearly\n",
    "- Format your final answer in \\\\boxed{} as just one letter (e.g., \\\\boxed{A})\n",
    "\n",
    "Your response should end with your final answer in the format \\\\boxed{X} where X is the correct option.\"\"\"\n",
    "        elif question_type == \"MCQ(multiple)\":\n",
    "            prompt += \"\"\"This is a multiple choice question where multiple options can be correct. Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "\n",
    "For this question:\n",
    "- Choose ONE OR MORE options (A, B, C, and/or D)\n",
    "- Show your reasoning clearly\n",
    "- Format your final answer in \\\\boxed{} with letters (e.g., \\\\boxed{ABC} or \\\\boxed{B})\n",
    "\n",
    "Your response should end with your final answer in the format \\\\boxed{X} where X contains all correct options.\"\"\"\n",
    "        elif question_type == \"Integer\":\n",
    "            prompt += \"\"\"This is a numerical question. Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "\n",
    "For this question:\n",
    "- Provide a numerical value\n",
    "- Show your complete calculation\n",
    "- Round to appropriate decimal places if needed\n",
    "- Format your final answer in \\\\boxed{} (e.g., \\\\boxed{2.5} or \\\\boxed{42})\n",
    "\n",
    "Your response should end with your final answer in the format \\\\boxed{X} where X is the numerical answer.\"\"\"\n",
    "        else:\n",
    "            # Default case\n",
    "            prompt += \"\"\"Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "Show your complete reasoning and format your final answer in \\\\boxed{} (e.g., \\\\boxed{A} for MCQ or \\\\boxed{42} for numerical)\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def extract_answer(self, response_text: str, question_type: str) -> str:\n",
    "        \"\"\"Extract the final answer from model response\"\"\"\n",
    "        try:\n",
    "            # First try to find boxed answer\n",
    "            boxed_match = re.search(r'\\\\boxed\\{([^}]+)\\}', response_text)\n",
    "            if boxed_match:\n",
    "                answer = boxed_match.group(1).strip()\n",
    "            else:\n",
    "                # Try other patterns\n",
    "                answer_patterns = [\n",
    "                    r'\\*\\*Answer:\\*\\*\\s*(.+)',\n",
    "                    r'Answer:\\s*(.+)',\n",
    "                    r'Final answer:\\s*(.+)',\n",
    "                    r'The answer is:\\s*(.+)',\n",
    "                    r'Therefore,?\\s*(.+)',\n",
    "                ]\n",
    "                answer = None\n",
    "                for pattern in answer_patterns:\n",
    "                    match = re.search(pattern, response_text, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        answer = match.group(1).strip()\n",
    "                        break\n",
    "                \n",
    "                if not answer:\n",
    "                    # Take last line as fallback\n",
    "                    lines = response_text.strip().split('\\n')\n",
    "                    answer = lines[-1].strip()\n",
    "            \n",
    "            # Clean up answer based on question type\n",
    "            if question_type == \"MCQ\":\n",
    "                # Extract single letter\n",
    "                match = re.search(r'[ABCD]', answer.upper())\n",
    "                return match.group(0) if match else answer[:10]\n",
    "            elif question_type == \"MCQ(multiple)\":\n",
    "                # Extract multiple letters\n",
    "                letters = re.findall(r'[ABCD]', answer.upper())\n",
    "                unique_letters = sorted(set(letters))\n",
    "                return ''.join(unique_letters) if unique_letters else answer[:20]\n",
    "            elif question_type == \"Integer\":\n",
    "                # Extract number\n",
    "                number_match = re.search(r'-?\\d+\\.?\\d*', answer)\n",
    "                return number_match.group(0) if number_match else answer[:20]\n",
    "            \n",
    "            return answer[:50]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error extracting answer: {e}\")\n",
    "            return response_text[:50]\n",
    "    \n",
    "    def is_answer_correct(self, predicted_answer: str, question_data: pd.Series) -> bool:\n",
    "        \"\"\"Check if predicted answer is correct\"\"\"\n",
    "        try:\n",
    "            predicted = str(predicted_answer).strip().upper()\n",
    "            correct = str(question_data['gold']).strip().upper()\n",
    "            question_type = question_data['type']\n",
    "            \n",
    "            if question_type == \"MCQ\":\n",
    "                return predicted == correct\n",
    "            elif question_type == \"MCQ(multiple)\":\n",
    "                # Handle multiple choice with multiple correct answers\n",
    "                pred_letters = set(re.findall(r'[ABCD]', predicted))\n",
    "                correct_letters = set(re.findall(r'[ABCD]', correct))\n",
    "                return pred_letters == correct_letters\n",
    "            elif question_type == \"Integer\":\n",
    "                # First try exact match\n",
    "                if predicted == correct:\n",
    "                    return True\n",
    "                # Try numerical comparison with tolerance\n",
    "                try:\n",
    "                    pred_num = float(predicted)\n",
    "                    correct_num = float(correct)\n",
    "                    tolerance = abs(correct_num) * 0.01 if abs(correct_num) > 1 else 0.01\n",
    "                    return abs(pred_num - correct_num) <= tolerance\n",
    "                except ValueError:\n",
    "                    return predicted == correct\n",
    "            \n",
    "            # Default exact match\n",
    "            return predicted == correct\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error comparing answers: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def evaluate_single_question(self, question_data: pd.Series, run_id: int, question_idx: int) -> Optional[Dict]:\n",
    "        \"\"\"Evaluate a single question using Qwen 2.5 VL\"\"\"\n",
    "        try:\n",
    "            prompt = self.create_question_prompt(question_data)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response_text = self.client.generate_content(prompt)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            if not response_text:\n",
    "                logger.error(f\"‚ùå Failed to get response for question {question_data.get('index', question_idx)}\")\n",
    "                return None\n",
    "            \n",
    "            predicted_answer = self.extract_answer(response_text, question_data['type'])\n",
    "            is_correct = self.is_answer_correct(predicted_answer, question_data)\n",
    "            \n",
    "            # Log each completion\n",
    "            status = \"‚úÖ CORRECT\" if is_correct else \"‚ùå WRONG\"\n",
    "            logger.info(f\"Run {run_id} | Q{question_idx+1}/{len(self.df)}: {status} ({inference_time:.1f}s) | Predicted: {predicted_answer} | Gold: {question_data['gold']}\")\n",
    "            \n",
    "            return {\n",
    "                'run_id': run_id,\n",
    "                'question_idx': question_idx,\n",
    "                'dataset_index': question_data.get('index', question_idx),\n",
    "                'subject': question_data['subject'],\n",
    "                'question_type': question_data['type'],\n",
    "                'question_text': question_data['question'][:200] + \"...\" if len(question_data['question']) > 200 else question_data['question'],\n",
    "                'correct_answer': question_data['gold'],\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'is_correct': is_correct,\n",
    "                'inference_time': inference_time,\n",
    "                'full_response': response_text[:1000] + \"...\" if len(response_text) > 1000 else response_text  # Truncate for storage\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error evaluating question {question_data.get('index', question_idx)}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_single_evaluation_run(self, run_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Run a single evaluation run (sequential processing) with proper resume support\"\"\"\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"üöÄ Starting Run {run_id}/{self.num_runs}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        # Check if we're resuming this run\n",
    "        is_resuming_run = (run_id == self.state.current_run and \n",
    "                          self.state.current_run_question_idx > 0)\n",
    "        \n",
    "        if is_resuming_run:\n",
    "            # Resume from existing state\n",
    "            logger.info(f\"üîÑ Resuming Run {run_id} from question {self.state.current_run_question_idx + 1}\")\n",
    "            results = self.state.current_run_results.copy()\n",
    "            shuffled_indices = self.state.current_run_shuffled_indices\n",
    "            start_idx = self.state.current_run_question_idx\n",
    "        else:\n",
    "            # Start new run\n",
    "            logger.info(f\"üÜï Starting new Run {run_id}\")\n",
    "            # Shuffle questions for this run\n",
    "            shuffled_indices = list(range(len(self.df)))\n",
    "            random.seed(run_id)  # Consistent shuffle for each run\n",
    "            random.shuffle(shuffled_indices)\n",
    "            \n",
    "            # Update state for new run\n",
    "            self.state.current_run = run_id\n",
    "            self.state.current_run_question_idx = 0\n",
    "            self.state.current_run_results = []\n",
    "            self.state.current_run_shuffled_indices = shuffled_indices\n",
    "            \n",
    "            results = []\n",
    "            start_idx = 0\n",
    "        \n",
    "        run_start_time = time.time()\n",
    "        \n",
    "        # Process questions sequentially starting from the resume point\n",
    "        for idx in range(start_idx, len(shuffled_indices)):\n",
    "            if self.stop_requested:\n",
    "                logger.info(\"‚èπÔ∏è Stop requested, ending run early\")\n",
    "                break\n",
    "            \n",
    "            # Get the actual question data using shuffled index\n",
    "            question_data = self.df.iloc[shuffled_indices[idx]]\n",
    "            \n",
    "            result = self.evaluate_single_question(question_data, run_id, idx)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update state immediately\n",
    "                self.state.current_run_results = results.copy()\n",
    "                self.state.current_run_question_idx = idx\n",
    "                self.state.completed_questions += 1\n",
    "            \n",
    "            # Save state every 5 questions for minimal data loss\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                self.save_state()\n",
    "                progress = ((idx + 1) / len(shuffled_indices)) * 100\n",
    "                logger.info(f\"üìä Run {run_id} Progress: {progress:.1f}% ({idx + 1}/{len(shuffled_indices)} questions) [State Saved]\")\n",
    "            \n",
    "            # Also save partial results every 25 questions\n",
    "            if (idx + 1) % 25 == 0:\n",
    "                self._save_partial_run_results(run_id, results, idx + 1)\n",
    "        \n",
    "        run_duration = time.time() - run_start_time\n",
    "        \n",
    "        if not results:\n",
    "            logger.error(f\"‚ùå No valid results for run {run_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_count = sum(1 for r in results if r['is_correct'])\n",
    "        accuracy = (correct_count / len(results)) * 100\n",
    "        \n",
    "        # Create run summary\n",
    "        run_summary = {\n",
    "            'run_id': run_id,\n",
    "            'model_name': self.model_name,\n",
    "            'total_questions': len(results),\n",
    "            'correct_answers': correct_count,\n",
    "            'accuracy': accuracy,\n",
    "            'duration': run_duration,\n",
    "            'avg_time_per_question': run_duration / len(results),\n",
    "            'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        # Reset run state since this run is complete\n",
    "        self.state.current_run_question_idx = 0\n",
    "        self.state.current_run_results = []\n",
    "        self.state.current_run_shuffled_indices = []\n",
    "        \n",
    "        logger.info(f\"‚úÖ Run {run_id} completed: {accuracy:.2f}% accuracy ({correct_count}/{len(results)}) in {run_duration:.1f}s\")\n",
    "        logger.info(f\"‚è±Ô∏è Average time per question: {run_summary['avg_time_per_question']:.1f}s\")\n",
    "        \n",
    "        return run_summary\n",
    "    \n",
    "    def save_run_results(self, run_summary: Dict):\n",
    "        \"\"\"Save results for a single run to completed_runs directory\"\"\"\n",
    "        timestamp = run_summary['timestamp']\n",
    "        filename = f\"jeebench_qwen25vl_run_{run_summary['run_id']:02d}_{timestamp}.json\"\n",
    "        filepath = self.results_dir / \"completed_runs\" / filename\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(run_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"üíæ Run {run_summary['run_id']} results saved to: completed_runs/{filename}\")\n",
    "        \n",
    "        # Clean up partial results for this run\n",
    "        self._cleanup_partial_results(run_summary['run_id'])\n",
    "    \n",
    "    def _cleanup_partial_results(self, run_id: int):\n",
    "        \"\"\"Clean up partial result files after a run is completed\"\"\"\n",
    "        partial_dir = self.results_dir / \"partial_results\"\n",
    "        if partial_dir.exists():\n",
    "            # Find and remove partial files for this run\n",
    "            for partial_file in partial_dir.glob(f\"*_run_{run_id:02d}_partial_*\"):\n",
    "                try:\n",
    "                    partial_file.unlink()\n",
    "                    logger.debug(f\"üóëÔ∏è Cleaned up partial file: {partial_file.name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"‚ö†Ô∏è Could not clean up partial file {partial_file}: {e}\")\n",
    "    \n",
    "    def calculate_statistics(self, all_run_summaries: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate overall statistics across all runs\"\"\"\n",
    "        accuracies = [summary['accuracy'] for summary in all_run_summaries]\n",
    "        \n",
    "        return {\n",
    "            'num_runs': len(accuracies),\n",
    "            'mean_accuracy': np.mean(accuracies),\n",
    "            'std_accuracy': np.std(accuracies, ddof=1) if len(accuracies) > 1 else 0,\n",
    "            'sem_accuracy': stats.sem(accuracies) if len(accuracies) > 1 else 0,\n",
    "            'min_accuracy': np.min(accuracies),\n",
    "            'max_accuracy': np.max(accuracies),\n",
    "            'confidence_interval_95': stats.t.interval(\n",
    "                0.95, len(accuracies) - 1,\n",
    "                loc=np.mean(accuracies),\n",
    "                scale=stats.sem(accuracies)\n",
    "            ) if len(accuracies) > 1 else (np.mean(accuracies), np.mean(accuracies)),\n",
    "            'individual_accuracies': accuracies\n",
    "        }\n",
    "    \n",
    "    def analyze_convergence_and_variance(self, all_run_summaries: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze convergence for optimal k determination\"\"\"\n",
    "        accuracies = [summary['accuracy'] for summary in all_run_summaries]\n",
    "        n_runs = len(accuracies)\n",
    "        \n",
    "        convergence_analysis = {\n",
    "            'k_values': [],\n",
    "            'running_means': [],\n",
    "            'running_stds': [],\n",
    "            'running_sems': [],\n",
    "            'confidence_intervals': [],\n",
    "            'relative_changes': [],\n",
    "            'stability_metrics': [],\n",
    "            'cost_effectiveness': []\n",
    "        }\n",
    "        \n",
    "        for k in range(3, n_runs + 1):\n",
    "            subset_accuracies = accuracies[:k]\n",
    "            \n",
    "            mean_acc = np.mean(subset_accuracies)\n",
    "            std_acc = np.std(subset_accuracies, ddof=1) if k > 1 else 0\n",
    "            sem_acc = stats.sem(subset_accuracies) if k > 1 else 0\n",
    "            \n",
    "            if k > 1:\n",
    "                ci = stats.t.interval(0.95, k-1, loc=mean_acc, scale=sem_acc) if sem_acc > 0 else (mean_acc, mean_acc)\n",
    "                ci_width = ci[1] - ci[0]\n",
    "            else:\n",
    "                ci = (mean_acc, mean_acc)\n",
    "                ci_width = 0\n",
    "            \n",
    "            if k > 3:\n",
    "                prev_mean = convergence_analysis['running_means'][-1]\n",
    "                relative_change = abs(mean_acc - prev_mean) / prev_mean * 100 if prev_mean > 0 else 0\n",
    "            else:\n",
    "                relative_change = np.inf\n",
    "            \n",
    "            cv = std_acc / mean_acc * 100 if mean_acc > 0 else np.inf\n",
    "            cost_effectiveness = ci_width * k\n",
    "            \n",
    "            convergence_analysis['k_values'].append(k)\n",
    "            convergence_analysis['running_means'].append(mean_acc)\n",
    "            convergence_analysis['running_stds'].append(std_acc)\n",
    "            convergence_analysis['running_sems'].append(sem_acc)\n",
    "            convergence_analysis['confidence_intervals'].append(ci)\n",
    "            convergence_analysis['relative_changes'].append(relative_change)\n",
    "            convergence_analysis['stability_metrics'].append(cv)\n",
    "            convergence_analysis['cost_effectiveness'].append(cost_effectiveness)\n",
    "        \n",
    "        return convergence_analysis\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        \"\"\"Run complete evaluation with resume capability\"\"\"\n",
    "        logger.info(\"üéØ Starting JEEBench Qwen 2.5 VL 7B Evaluation with Resume Capability\")\n",
    "        logger.info(f\"üìö Dataset: {len(self.df)} questions\")\n",
    "        logger.info(f\"ü§ñ Model: {self.model_name}\")\n",
    "        logger.info(f\"üîÑ Total runs planned: {self.num_runs}\")\n",
    "        \n",
    "        try:\n",
    "            # Resume from where we left off\n",
    "            for run_id in range(self.state.current_run, self.num_runs + 1):\n",
    "                if self.stop_requested:\n",
    "                    break\n",
    "                \n",
    "                # Run single evaluation\n",
    "                run_summary = self.run_single_evaluation_run(run_id)\n",
    "                \n",
    "                if run_summary:\n",
    "                    # Save run results\n",
    "                    self.save_run_results(run_summary)\n",
    "                    \n",
    "                    # Update state\n",
    "                    self.state.all_run_summaries.append(run_summary)\n",
    "                    self.state.current_run = run_id + 1\n",
    "                    \n",
    "                    # Save state after each run\n",
    "                    self.save_state()\n",
    "                    \n",
    "                    # Print progress\n",
    "                    progress = (run_id / self.num_runs) * 100\n",
    "                    elapsed = time.time() - self.state.start_time\n",
    "                    eta = (elapsed / run_id) * (self.num_runs - run_id) if run_id > 0 else 0\n",
    "                    \n",
    "                    if len(self.state.all_run_summaries) > 0:\n",
    "                        avg_accuracy = np.mean([s['accuracy'] for s in self.state.all_run_summaries])\n",
    "                        logger.info(f\"üìä Progress: {progress:.1f}% | ETA: {eta/3600:.1f}h | Avg accuracy so far: {avg_accuracy:.2f}%\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"‚èπÔ∏è Evaluation interrupted by user\")\n",
    "            self.interrupted = True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"‚ùå Error during evaluation: {e}\")\n",
    "        finally:\n",
    "            # Always save final state\n",
    "            self.save_state()\n",
    "            \n",
    "            # Generate final report if we have results\n",
    "            if self.state.all_run_summaries:\n",
    "                self.generate_final_report()\n",
    "        \n",
    "        logger.info(\"üèÅ Evaluation session ended!\")\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"Generate comprehensive final report\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_file = self.results_dir / f\"jeebench_qwen25vl_final_report_{timestamp}.txt\"\n",
    "        \n",
    "        stats = self.calculate_statistics(self.state.all_run_summaries)\n",
    "        convergence_data = self.analyze_convergence_and_variance(self.state.all_run_summaries)\n",
    "        \n",
    "        # Collect all results for detailed analysis\n",
    "        all_results = []\n",
    "        for run_summary in self.state.all_run_summaries:\n",
    "            all_results.extend(run_summary['results'])\n",
    "        \n",
    "        report_content = f\"\"\"JEEBench Qwen 2.5 VL 7B Evaluation Report\n",
    "{'='*80}\n",
    "\n",
    "Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Model: {self.model_name}\n",
    "Dataset: JEEBench ({len(self.df)} questions)\n",
    "Completed Runs: {len(self.state.all_run_summaries)}/{self.num_runs}\n",
    "Evaluation Mode: Sequential (Single GPU)\n",
    "\n",
    "PERFORMANCE SUMMARY\n",
    "{'-'*40}\n",
    "Mean Accuracy: {stats['mean_accuracy']:.2f}% ¬± {stats['std_accuracy']:.2f}%\n",
    "Standard Error: {stats['sem_accuracy']:.2f}%\n",
    "95% Confidence Interval: [{stats['confidence_interval_95'][0]:.2f}%, {stats['confidence_interval_95'][1]:.2f}%]\n",
    "Range: {stats['min_accuracy']:.2f}% - {stats['max_accuracy']:.2f}%\n",
    "\n",
    "PERFORMANCE BREAKDOWN\n",
    "{'-'*40}\n",
    "\"\"\"\n",
    "        \n",
    "        # Performance by category\n",
    "        categories = ['subject', 'question_type']\n",
    "        for category in categories:\n",
    "            report_content += f\"\\nBy {category.title()}:\\n\"\n",
    "            category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "            \n",
    "            for result in all_results:\n",
    "                cat_value = result[category]\n",
    "                category_stats[cat_value]['total'] += 1\n",
    "                if result['is_correct']:\n",
    "                    category_stats[cat_value]['correct'] += 1\n",
    "            \n",
    "            for cat_value, stats_dict in sorted(category_stats.items()):\n",
    "                accuracy = (stats_dict['correct'] / stats_dict['total']) * 100\n",
    "                report_content += f\"  {cat_value}: {accuracy:.2f}% ({stats_dict['correct']}/{stats_dict['total']})\\n\"\n",
    "        \n",
    "        # Add individual run accuracies\n",
    "        report_content += f\"\\nINDIVIDUAL RUN ACCURACIES\\n{'-'*40}\\n\"\n",
    "        for i, accuracy in enumerate(stats['individual_accuracies'], 1):\n",
    "            report_content += f\"Run {i}: {accuracy:.2f}%\\n\"\n",
    "        \n",
    "        # Add timing information\n",
    "        if all_results:\n",
    "            avg_inference_time = np.mean([r['inference_time'] for r in all_results])\n",
    "            total_inference_time = sum([r['inference_time'] for r in all_results])\n",
    "            report_content += f\"\\nTIMING ANALYSIS\\n{'-'*40}\\n\"\n",
    "            report_content += f\"Average inference time per question: {avg_inference_time:.2f}s\\n\"\n",
    "            report_content += f\"Total inference time: {total_inference_time/3600:.2f}h\\n\"\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        logger.info(f\"üìÑ Final report saved to: {report_file}\")\n",
    "        logger.info(f\"üéØ Final Pass@1 Accuracy: {stats['mean_accuracy']:.2f}% ¬± {stats['std_accuracy']:.2f}%\")\n",
    "\n",
    "# Configuration for Jupyter Notebook\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"qwen/qwen2.5-vl-7b\"  # Change this if your model has a different name in LM Studio\n",
    "NUM_RUNS = 10\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"Main evaluation function for Jupyter\"\"\"\n",
    "    evaluator = JEEBenchQwen25VLEvaluator(NUM_RUNS, MODEL_NAME)\n",
    "    evaluator.run_evaluation()\n",
    "\n",
    "def resume_evaluation():\n",
    "    \"\"\"Resume evaluation from saved state\"\"\"\n",
    "    evaluator = JEEBenchQwen25VLEvaluator(NUM_RUNS, MODEL_NAME)\n",
    "    logger.info(\"üîÑ Resuming evaluation from saved state...\")\n",
    "    evaluator.run_evaluation()\n",
    "\n",
    "def check_progress():\n",
    "    \"\"\"Check current progress without running evaluation\"\"\"\n",
    "    evaluator = JEEBenchQwen25VLEvaluator(NUM_RUNS, MODEL_NAME)\n",
    "    state = evaluator.state\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CURRENT EVALUATION PROGRESS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Current Run: {state.current_run}/{NUM_RUNS}\")\n",
    "    \n",
    "    if state.current_run_question_idx > 0:\n",
    "        print(f\"Current Run Progress: Question {state.current_run_question_idx + 1}/{len(evaluator.df)} within Run {state.current_run}\")\n",
    "        current_run_progress = ((state.current_run_question_idx + 1) / len(evaluator.df)) * 100\n",
    "        print(f\"Current Run Progress: {current_run_progress:.1f}%\")\n",
    "    \n",
    "    print(f\"Total Completed Questions: {state.completed_questions:,}/{state.total_questions:,}\")\n",
    "    print(f\"Overall Progress: {(state.completed_questions/state.total_questions)*100:.1f}%\")\n",
    "    \n",
    "    if state.all_run_summaries:\n",
    "        accuracies = [s['accuracy'] for s in state.all_run_summaries]\n",
    "        print(f\"Completed Runs: {len(state.all_run_summaries)}\")\n",
    "        print(f\"Average Accuracy: {np.mean(accuracies):.2f}% ¬± {np.std(accuracies, ddof=1):.2f}%\")\n",
    "        \n",
    "        elapsed = time.time() - state.start_time\n",
    "        if len(state.all_run_summaries) > 0:\n",
    "            eta = (elapsed / len(state.all_run_summaries)) * (NUM_RUNS - len(state.all_run_summaries))\n",
    "            print(f\"Elapsed Time: {elapsed/3600:.1f}h\")\n",
    "            print(f\"Estimated Time Remaining: {eta/3600:.1f}h\")\n",
    "    \n",
    "    # Show current run results if any\n",
    "    if state.current_run_results:\n",
    "        current_correct = sum(1 for r in state.current_run_results if r['is_correct'])\n",
    "        current_accuracy = (current_correct / len(state.current_run_results)) * 100\n",
    "        print(f\"Current Run {state.current_run} so far: {current_accuracy:.1f}% ({current_correct}/{len(state.current_run_results)})\")\n",
    "    \n",
    "    print(f\"Last Save: {datetime.fromtimestamp(state.last_save_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"State File: {evaluator.state_file}\")\n",
    "    \n",
    "    # Check for partial results\n",
    "    partial_dir = evaluator.results_dir / \"partial_results\"\n",
    "    if partial_dir.exists():\n",
    "        partial_files = list(partial_dir.glob(\"*.json\"))\n",
    "        if partial_files:\n",
    "            print(f\"Partial Result Files: {len(partial_files)}\")\n",
    "            print(\"(These will be cleaned up when runs complete)\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "def recover_from_partial_results():\n",
    "    \"\"\"Attempt to recover progress from partial result files if state is corrupted\"\"\"\n",
    "    evaluator = JEEBenchQwen25VLEvaluator(NUM_RUNS, MODEL_NAME)\n",
    "    partial_dir = evaluator.results_dir / \"partial_results\"\n",
    "    \n",
    "    if not partial_dir.exists():\n",
    "        print(\"‚ùå No partial results directory found\")\n",
    "        return\n",
    "    \n",
    "    partial_files = list(partial_dir.glob(\"*.json\"))\n",
    "    if not partial_files:\n",
    "        print(\"‚ùå No partial result files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"üîç Found {len(partial_files)} partial result files:\")\n",
    "    for pf in sorted(partial_files):\n",
    "        try:\n",
    "            with open(pf, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"  üìÑ {pf.name}: Run {data.get('run_id', '?')}, {data.get('questions_completed', '?')} questions, {data.get('partial_accuracy', '?'):.1f}% accuracy\")\n",
    "        except Exception as e:\n",
    "            print(f\"  ‚ùå {pf.name}: Error reading file - {e}\")\n",
    "    \n",
    "    print(\"\\nüí° You can manually review these files if needed for data recovery\")\n",
    "    print(\"üîÑ Try running resume_evaluation() to continue from the last saved state\")\n",
    "\n",
    "def reset_evaluation():\n",
    "    \"\"\"Reset evaluation state (use with caution!)\"\"\"\n",
    "    evaluator = JEEBenchQwen25VLEvaluator(NUM_RUNS, MODEL_NAME)\n",
    "    \n",
    "    print(\"‚ö†Ô∏è  WARNING: This will delete all progress and start fresh!\")\n",
    "    confirm = input(\"Type 'RESET' to confirm: \")\n",
    "    \n",
    "    if confirm == \"RESET\":\n",
    "        files_to_remove = []\n",
    "        \n",
    "        # Main state file\n",
    "        if evaluator.state_file.exists():\n",
    "            files_to_remove.append(evaluator.state_file)\n",
    "        \n",
    "        # Backup state file\n",
    "        backup_file = evaluator.state_file.with_suffix('.backup')\n",
    "        if backup_file.exists():\n",
    "            files_to_remove.append(backup_file)\n",
    "        \n",
    "        # Remove all files\n",
    "        for file_path in files_to_remove:\n",
    "            try:\n",
    "                file_path.unlink()\n",
    "                print(f\"‚úÖ Removed: {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error removing {file_path.name}: {e}\")\n",
    "        \n",
    "        if files_to_remove:\n",
    "            print(\"‚úÖ Evaluation state reset successfully!\")\n",
    "        else:\n",
    "            print(\"‚ÑπÔ∏è  No existing state files found.\")\n",
    "    else:\n",
    "        print(\"‚ùå Reset cancelled.\")\n",
    "\n",
    "def force_upgrade_state():\n",
    "    \"\"\"Force upgrade of old state file format\"\"\"\n",
    "    try:\n",
    "        evaluator = JEEBenchQwen25VLEvaluator(NUM_RUNS, MODEL_NAME)\n",
    "        print(\"‚úÖ State file upgraded successfully!\")\n",
    "        check_progress()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error upgrading state: {e}\")\n",
    "        print(\"üí° Try reset_evaluation() if the issue persists\")\n",
    "\n",
    "def load_jeebench_sample(n_samples: int = 5):\n",
    "    \"\"\"Load and display a sample of JEEBench questions for inspection\"\"\"\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\"üìö Loading JEEBench dataset...\")\n",
    "    ds = load_dataset(\"daman1209arora/jeebench\")\n",
    "    df = pd.DataFrame(ds['test'])\n",
    "    \n",
    "    print(f\"\\nüìä Dataset Info:\")\n",
    "    print(f\"Total questions: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Question types: {df['type'].value_counts().to_dict()}\")\n",
    "    print(f\"Subjects: {df['subject'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(f\"\\nüîç Sample {n_samples} questions:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_df = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nüìù Question {idx + 1}:\")\n",
    "        print(f\"Subject: {row['subject']}\")\n",
    "        print(f\"Type: {row['type']}\")\n",
    "        print(f\"Question: {row['question'][:200]}{'...' if len(row['question']) > 200 else ''}\")\n",
    "        print(f\"Gold Answer: {row['gold']}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def test_model_connection():\n",
    "    \"\"\"Test if Qwen 2.5 VL model is accessible via LM Studio\"\"\"\n",
    "    try:\n",
    "        print(\"üîÑ Testing Qwen 2.5 VL model connection...\")\n",
    "        model = lms.llm(MODEL_NAME)\n",
    "        \n",
    "        test_prompt = \"Hello! Can you solve this simple math problem: What is 2 + 2?\"\n",
    "        response = model.respond(test_prompt)\n",
    "        \n",
    "        print(\"‚úÖ Model connection successful!\")\n",
    "        print(f\"üìù Test response: {response}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Model connection failed: {e}\")\n",
    "        print(\"üîß Please ensure:\")\n",
    "        print(\"  1. LM Studio is running\")\n",
    "        print(\"  2. Qwen 2.5 VL 7B model is loaded\")\n",
    "        print(\"  3. API server is enabled in LM Studio\")\n",
    "        print(f\"  4. Model name '{MODEL_NAME}' is correct\")\n",
    "        return False\n",
    "\n",
    "\"\"\"\n",
    "# 1. First, test if the model is accessible:\n",
    "test_model_connection()\n",
    "\n",
    "# 2. To inspect the dataset first:\n",
    "df = load_jeebench_sample(10)\n",
    "\n",
    "# 3. To start new evaluation:\n",
    "run_evaluation()\n",
    "\n",
    "# 4. To resume from where you left off:\n",
    "resume_evaluation()\n",
    "\n",
    "# 5. To check current progress:\n",
    "check_progress()\n",
    "\n",
    "# 6. If you get state compatibility errors, try:\n",
    "force_upgrade_state()\n",
    "\n",
    "# 7. To recover from partial results if state is corrupted:\n",
    "recover_from_partial_results()\n",
    "\n",
    "# 8. To reset everything (use carefully!):\n",
    "reset_evaluation()\n",
    "\n",
    "# Example of running a single question for testing:\n",
    "def test_single_question():\n",
    "    evaluator = JEEBenchQwen25VLEvaluator(1, MODEL_NAME)\n",
    "    sample_question = evaluator.df.iloc[0]\n",
    "    result = evaluator.evaluate_single_question(sample_question, 1, 0)\n",
    "    print(\"Test result:\", result)\n",
    "    return result\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4633052e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:54:54,786 - INFO - {\"client\": \"<lmstudio.sync_api.Client object at 0x0000026C52D90740>\", \"event\": \"Websocket handling thread started\", \"thread_id\": \"Thread-3\"}\n",
      "2025-07-28 16:54:54,789 - INFO - {\"event\": \"Websocket handling task started\", \"ws_url\": \"ws://localhost:1234/llm\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîÑ Testing Qwen 2.5 VL model connection...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 16:54:55,265 - INFO - HTTP Request: GET ws://localhost:1234/llm \"HTTP/1.1 101 Switching Protocols\"\n",
      "2025-07-28 16:54:55,266 - INFO - {\"event\": \"Websocket session established (ws://localhost:1234/llm)\", \"ws_url\": \"ws://localhost:1234/llm\"}\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Model connection successful!\n",
      "üìù Test response: Certainly! The answer to the equation \\(2 + 2\\) is:\n",
      "\n",
      "\\[4\\]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_connection()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
