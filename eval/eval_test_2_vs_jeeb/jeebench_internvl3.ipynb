{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b6420416",
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmstudio as lms\n",
    "model = lms.llm()\n",
    "prediction = model.respond(\"What is the meaning of life?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8f071513",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The question \"What is the meaning of life?\" has been pondered by philosophers, theologians, scientists, and thinkers throughout history. There isn\\'t a single definitive answer as it can vary greatly depending on cultural, religious, philosophical, and personal perspectives.\\n\\n1. **Philosophical Perspectives**: Philosophers have proposed various answers to this question. Existentialists like Jean-Paul Sartre argue that life has no inherent meaning, and it is up to each individual to create their own purpose through choices and actions. In contrast, some philosophers believe in a predetermined or universal meaning of life.\\n\\n2. **Religious Perspectives**: Many religions offer answers based on divine will or spiritual fulfillment. For example, in Christianity, the meaning of life might be seen as serving God and preparing for eternal life with Him. Similarly, in Buddhism, it is often about achieving enlightenment (Nirvana) and escaping the cycle of rebirth.\\n\\n3. **Scientific Perspectives**: From a scientific standpoint, one could argue that the purpose of life is to survive and reproduce, ensuring the continuation of our species through natural selection. However, this perspective does not account for subjective experiences or personal fulfillment.\\n\\n4. **Personal Perspectives**: On an individual level, people often find meaning in relationships, achievements, passions, helping others, contributing to society, or pursuing happiness and well-being.\\n\\nUltimately, the meaning of life is a deeply personal question that each person must answer based on their own beliefs, values, experiences, and aspirations. It can be a lifelong journey of exploration and discovery.'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4673a0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n# 1. First, test if the model is accessible:\\ntest_model_connection()\\n\\n# 2. To inspect the dataset first:\\ndf = load_jeebench_sample(10)\\n\\n# 3. To start new evaluation:\\nrun_evaluation()\\n\\n# 4. To resume from where you left off:\\nresume_evaluation()\\n\\n# 5. To check current progress:\\ncheck_progress()\\n\\n# 6. If you get state compatibility errors, try:\\nforce_upgrade_state()\\n\\n# 7. To recover from partial results if state is corrupted:\\nrecover_from_partial_results()\\n\\n# 8. To reset everything (use carefully!):\\nreset_evaluation()\\n\\n# Example of running a single question for testing:\\ndef test_single_question():\\n    evaluator = JEEBenchInternVL3Evaluator(1, MODEL_NAME)\\n    sample_question = evaluator.df.iloc[0]\\n    result = evaluator.evaluate_single_question(sample_question, 1, 0)\\n    print(\"Test result:\", result)\\n    return result\\n\\n# Note: With the new implementation, state is saved every 5 questions\\n# and partial results are saved every 25 questions, so you\\'ll lose at most\\n# 4 questions of work if the evaluation is interrupted.\\n\\n# If you encounter \\'EvaluationState\\' object has no attribute errors:\\n# This happens when you have an old state file. Try force_upgrade_state()\\n# or reset_evaluation() to start fresh.\\n'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "import os\n",
    "from pathlib import Path\n",
    "import signal\n",
    "import sys\n",
    "from datetime import datetime\n",
    "import re\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from typing import List, Dict, Optional, Tuple\n",
    "import logging\n",
    "import pickle\n",
    "from dataclasses import dataclass, asdict\n",
    "from datasets import load_dataset\n",
    "import lmstudio as lms\n",
    "\n",
    "# Configure logging with UTF-8 encoding to handle Unicode characters\n",
    "logging.basicConfig(\n",
    "    level=logging.INFO,\n",
    "    format='%(asctime)s - %(levelname)s - %(message)s',\n",
    "    handlers=[\n",
    "        logging.FileHandler('jeebench_internvl3_evaluation.log', encoding='utf-8'),\n",
    "        logging.StreamHandler()\n",
    "    ]\n",
    ")\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "@dataclass\n",
    "class EvaluationState:\n",
    "    \"\"\"Persistent state for resuming evaluation\"\"\"\n",
    "    current_run: int\n",
    "    current_run_question_idx: int  # Which question within the current run\n",
    "    completed_questions: int\n",
    "    total_questions: int\n",
    "    all_run_summaries: List[Dict]\n",
    "    failed_questions: List[Dict]\n",
    "    current_run_results: List[Dict]  # Results for the current incomplete run\n",
    "    current_run_shuffled_indices: List[int]  # Shuffled indices for current run\n",
    "    start_time: float\n",
    "    last_save_time: float\n",
    "\n",
    "class InternVL3Client:\n",
    "    \"\"\"Local InternVL3 8B client via LM Studio API\"\"\"\n",
    "    \n",
    "    def __init__(self, model_name: str = \"internvl3-8b-instruct\"):\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        try:\n",
    "            # Initialize LM Studio model\n",
    "            self.model = lms.llm(model_name)\n",
    "            logger.info(f\"âœ… InternVL3 8B model loaded successfully: {model_name}\")\n",
    "            \n",
    "            # Test the model with a simple query\n",
    "            test_response = self.model.respond(\"Hello, can you see this?\")\n",
    "            test_response = test_response.content\n",
    "            logger.info(f\"ðŸ“ Model test response: {test_response[:100]}...\")\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Failed to initialize InternVL3 model: {e}\")\n",
    "            raise ValueError(f\"Could not load InternVL3 model '{model_name}'. Please ensure LM Studio is running and the model is available.\")\n",
    "    \n",
    "    def generate_content(self, prompt: str, max_retries: int = 3) -> Optional[str]:\n",
    "        \"\"\"Generate content using InternVL3 model with retry logic\"\"\"\n",
    "        \n",
    "        for attempt in range(max_retries):\n",
    "            try:\n",
    "                logger.debug(f\"ðŸ”„ Generating content (attempt {attempt + 1}/{max_retries})\")\n",
    "                \n",
    "                # Use LM Studio API to get response\n",
    "                response = self.model.respond(prompt)\n",
    "                \n",
    "                if response and response.content:\n",
    "                    return response.content\n",
    "                else:\n",
    "                    logger.warning(f\"âš ï¸ Empty response on attempt {attempt + 1}\")\n",
    "                    \n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Error on attempt {attempt + 1}: {e}\")\n",
    "                if attempt == max_retries - 1:\n",
    "                    logger.error(f\"ðŸ’¥ All {max_retries} attempts failed\")\n",
    "                    return None\n",
    "                \n",
    "                # Wait before retry\n",
    "                time.sleep(2 ** attempt)\n",
    "        \n",
    "        return None\n",
    "\n",
    "class JEEBenchInternVL3Evaluator:\n",
    "    def __init__(self, num_runs: int = 10, model_name: str = \"internvl3-8b-instruct\"):\n",
    "        self.num_runs = num_runs\n",
    "        self.model_name = model_name\n",
    "        \n",
    "        # Load JEEBench dataset\n",
    "        logger.info(\"ðŸ“š Loading JEEBench dataset...\")\n",
    "        self.dataset = load_dataset(\"daman1209arora/jeebench\")\n",
    "        \n",
    "        # Convert to pandas DataFrame for easier manipulation\n",
    "        self.df = pd.DataFrame(self.dataset['test'])\n",
    "        logger.info(f\"ðŸ“Š Loaded JEEBench dataset with {len(self.df)} questions\")\n",
    "        \n",
    "        # Print dataset info\n",
    "        logger.info(f\"ðŸ“‹ Dataset columns: {list(self.df.columns)}\")\n",
    "        logger.info(f\"ðŸ“ Question types: {self.df['type'].value_counts().to_dict()}\")\n",
    "        logger.info(f\"ðŸ“š Subjects: {self.df['subject'].value_counts().to_dict()}\")\n",
    "        \n",
    "        # Initialize InternVL3 client\n",
    "        logger.info(\"ðŸš€ Initializing InternVL3 8B model...\")\n",
    "        self.client = InternVL3Client(model_name)\n",
    "        \n",
    "        # Results and state management\n",
    "        self.results_dir = Path(\"jeebench_internvl3_evaluation_results\")\n",
    "        self.results_dir.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Create subdirectories for organization\n",
    "        (self.results_dir / \"partial_results\").mkdir(exist_ok=True)\n",
    "        (self.results_dir / \"completed_runs\").mkdir(exist_ok=True)\n",
    "        (self.results_dir / \"state_backups\").mkdir(exist_ok=True)\n",
    "        \n",
    "        self.state_file = self.results_dir / \"evaluation_state.pkl\"\n",
    "        self.state = self.load_or_create_state()\n",
    "        \n",
    "        # Control flags\n",
    "        self.stop_requested = False\n",
    "        self.interrupted = False\n",
    "        \n",
    "        logger.info(f\"ðŸ’¾ Results will be saved to: {self.results_dir}\")\n",
    "        logger.info(f\"ðŸŽ¯ Running {num_runs} evaluations with InternVL3 8B\")\n",
    "    \n",
    "    def load_or_create_state(self) -> EvaluationState:\n",
    "        \"\"\"Load existing state or create new one with enhanced recovery\"\"\"\n",
    "        if self.state_file.exists():\n",
    "            try:\n",
    "                with open(self.state_file, 'rb') as f:\n",
    "                    state = pickle.load(f)\n",
    "                \n",
    "                # Handle backward compatibility for old state files\n",
    "                # Check if this is an old state format and upgrade it\n",
    "                if not hasattr(state, 'current_run_question_idx'):\n",
    "                    logger.info(\"ðŸ”„ Upgrading old state format...\")\n",
    "                    state.current_run_question_idx = 0\n",
    "                if not hasattr(state, 'current_run_results'):\n",
    "                    state.current_run_results = []\n",
    "                if not hasattr(state, 'current_run_shuffled_indices'):\n",
    "                    state.current_run_shuffled_indices = []\n",
    "                \n",
    "                # Save the upgraded state immediately\n",
    "                self.state = state\n",
    "                self.save_state()\n",
    "                \n",
    "                logger.info(f\"ðŸ”„ Resumed from Run {state.current_run}, Question {state.current_run_question_idx + 1} within run, Total: {state.completed_questions}/{state.total_questions}\")\n",
    "                return state\n",
    "            except Exception as e:\n",
    "                logger.error(f\"âŒ Error loading state: {e}\")\n",
    "                \n",
    "                # Try to load from backup\n",
    "                backup_file = self.state_file.with_suffix('.backup')\n",
    "                if backup_file.exists():\n",
    "                    try:\n",
    "                        with open(backup_file, 'rb') as f:\n",
    "                            state = pickle.load(f)\n",
    "                        \n",
    "                        # Handle backward compatibility for old backup files\n",
    "                        if not hasattr(state, 'current_run_question_idx'):\n",
    "                            logger.info(\"ðŸ”„ Upgrading old backup state format...\")\n",
    "                            state.current_run_question_idx = 0\n",
    "                        if not hasattr(state, 'current_run_results'):\n",
    "                            state.current_run_results = []\n",
    "                        if not hasattr(state, 'current_run_shuffled_indices'):\n",
    "                            state.current_run_shuffled_indices = []\n",
    "                        \n",
    "                        # Save the upgraded state\n",
    "                        self.state = state\n",
    "                        self.save_state()\n",
    "                            \n",
    "                        logger.info(f\"ðŸ”„ Recovered from backup: Run {state.current_run}, Question {state.current_run_question_idx + 1} within run, Total: {state.completed_questions}/{state.total_questions}\")\n",
    "                        return state\n",
    "                    except Exception as backup_error:\n",
    "                        logger.error(f\"âŒ Error loading backup state: {backup_error}\")\n",
    "        \n",
    "        # Create new state\n",
    "        logger.info(\"ðŸ†• Creating new evaluation state\")\n",
    "        return EvaluationState(\n",
    "            current_run=1,\n",
    "            current_run_question_idx=0,\n",
    "            completed_questions=0,\n",
    "            total_questions=len(self.df) * self.num_runs,\n",
    "            all_run_summaries=[],\n",
    "            failed_questions=[],\n",
    "            current_run_results=[],\n",
    "            current_run_shuffled_indices=[],\n",
    "            start_time=time.time(),\n",
    "            last_save_time=time.time()\n",
    "        )\n",
    "    \n",
    "    def _convert_to_json_serializable(self, obj):\n",
    "        \"\"\"Convert numpy/pandas types to JSON serializable types\"\"\"\n",
    "        if hasattr(obj, 'item'):  # numpy scalar\n",
    "            return obj.item()\n",
    "        elif hasattr(obj, 'tolist'):  # numpy array\n",
    "            return obj.tolist()\n",
    "        elif isinstance(obj, (np.integer, np.int64, np.int32)):\n",
    "            return int(obj)\n",
    "        elif isinstance(obj, (np.floating, np.float64, np.float32)):\n",
    "            return float(obj)\n",
    "        elif isinstance(obj, np.bool_):\n",
    "            return bool(obj)\n",
    "        elif isinstance(obj, dict):\n",
    "            return {key: self._convert_to_json_serializable(value) for key, value in obj.items()}\n",
    "        elif isinstance(obj, list):\n",
    "            return [self._convert_to_json_serializable(item) for item in obj]\n",
    "        else:\n",
    "            return obj\n",
    "\n",
    "    def _save_partial_run_results(self, run_id: int, results: List[Dict], questions_completed: int):\n",
    "        \"\"\"Save partial results during a run to prevent data loss\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        filename = f\"jeebench_internvl3_run_{run_id:02d}_partial_{questions_completed}q_{timestamp}.json\"\n",
    "        filepath = self.results_dir / \"partial_results\" / filename\n",
    "        \n",
    "        # Create partial results directory if it doesn't exist\n",
    "        filepath.parent.mkdir(exist_ok=True)\n",
    "        \n",
    "        # Calculate current accuracy for this partial run\n",
    "        correct_count = sum(1 for r in results if r['is_correct'])\n",
    "        accuracy = (correct_count / len(results)) * 100 if results else 0\n",
    "        \n",
    "        partial_summary = {\n",
    "            'run_id': int(run_id),\n",
    "            'model_name': str(self.model_name),\n",
    "            'questions_completed': int(questions_completed),\n",
    "            'total_questions_in_run': int(len(self.df)),\n",
    "            'partial_results_count': int(len(results)),\n",
    "            'correct_answers': int(correct_count),\n",
    "            'partial_accuracy': float(accuracy),\n",
    "            'timestamp': str(timestamp),\n",
    "            'is_partial': True,\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        try:\n",
    "            # Convert to JSON serializable format\n",
    "            serializable_summary = self._convert_to_json_serializable(partial_summary)\n",
    "            \n",
    "            with open(filepath, 'w', encoding='utf-8') as f:\n",
    "                json.dump(serializable_summary, f, indent=2, ensure_ascii=False)\n",
    "            \n",
    "            logger.info(f\"ðŸ’¾ Partial results saved: {questions_completed} questions, {accuracy:.1f}% accuracy\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error saving partial results: {e}\")\n",
    "            # Try to save a simplified version without full results\n",
    "            try:\n",
    "                simple_summary = {\n",
    "                    'run_id': int(run_id),\n",
    "                    'questions_completed': int(questions_completed),\n",
    "                    'correct_answers': int(correct_count),\n",
    "                    'partial_accuracy': float(accuracy),\n",
    "                    'timestamp': str(timestamp),\n",
    "                    'error': 'Full results could not be serialized'\n",
    "                }\n",
    "                simple_filename = f\"jeebench_internvl3_run_{run_id:02d}_partial_simple_{questions_completed}q_{timestamp}.json\"\n",
    "                simple_filepath = filepath.parent / simple_filename\n",
    "                with open(simple_filepath, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(simple_summary, f, indent=2, ensure_ascii=False)\n",
    "                logger.info(f\"ðŸ’¾ Simplified partial results saved as fallback\")\n",
    "            except Exception as fallback_error:\n",
    "                logger.error(f\"ðŸ’¥ Even simplified save failed: {fallback_error}\")\n",
    "\n",
    "    def save_state(self):\n",
    "        \"\"\"Save current evaluation state with enhanced error handling\"\"\"\n",
    "        self.state.last_save_time = time.time()\n",
    "        try:\n",
    "            # Use temporary file for atomic write\n",
    "            temp_file = self.state_file.with_suffix('.tmp')\n",
    "            with open(temp_file, 'wb') as f:\n",
    "                pickle.dump(self.state, f)\n",
    "            temp_file.replace(self.state_file)\n",
    "            \n",
    "            # Also create a backup of the state file\n",
    "            backup_file = self.state_file.with_suffix('.backup')\n",
    "            with open(backup_file, 'wb') as f:\n",
    "                pickle.dump(self.state, f)\n",
    "            \n",
    "            logger.debug(f\"ðŸ’¾ State saved: Run {self.state.current_run}, Question {self.state.current_run_question_idx + 1} within run, Total: {self.state.completed_questions}/{self.state.total_questions}\")\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error saving state: {e}\")\n",
    "            # Try to save to backup location\n",
    "            try:\n",
    "                emergency_backup = self.results_dir / f\"emergency_state_backup_{int(time.time())}.pkl\"\n",
    "                with open(emergency_backup, 'wb') as f:\n",
    "                    pickle.dump(self.state, f)\n",
    "                logger.info(f\"ðŸš¨ Emergency backup saved to: {emergency_backup}\")\n",
    "            except Exception as backup_error:\n",
    "                logger.error(f\"ðŸ’¥ Failed to create emergency backup: {backup_error}\")\n",
    "    \n",
    "    def create_question_prompt(self, question_data: pd.Series) -> str:\n",
    "        \"\"\"Create appropriate prompt based on question type\"\"\"\n",
    "        question_type = question_data['type']\n",
    "        question_text = question_data['question']\n",
    "        \n",
    "        # Base prompt with question\n",
    "        prompt = f\"You are an expert at solving JEE (Joint Entrance Examination) problems. Please solve this question step by step.\\n\\nQuestion: {question_text}\\n\\n\"\n",
    "        \n",
    "        # Add type-specific instructions\n",
    "        if question_type == \"MCQ\":\n",
    "            prompt += \"\"\"This is a multiple choice question. Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "\n",
    "For this question:\n",
    "- Choose exactly ONE option (A, B, C, or D)\n",
    "- Show your reasoning clearly\n",
    "- Format your final answer in \\\\boxed{} as just one letter (e.g., \\\\boxed{A})\n",
    "\n",
    "Your response should end with your final answer in the format \\\\boxed{X} where X is the correct option.\"\"\"\n",
    "        elif question_type == \"MCQ(multiple)\":\n",
    "            prompt += \"\"\"This is a multiple choice question where multiple options can be correct. Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "\n",
    "For this question:\n",
    "- Choose ONE OR MORE options (A, B, C, and/or D)\n",
    "- Show your reasoning clearly\n",
    "- Format your final answer in \\\\boxed{} with letters (e.g., \\\\boxed{ABC} or \\\\boxed{B})\n",
    "\n",
    "Your response should end with your final answer in the format \\\\boxed{X} where X contains all correct options.\"\"\"\n",
    "        elif question_type == \"Integer\":\n",
    "            prompt += \"\"\"This is a numerical question. Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "\n",
    "For this question:\n",
    "- Provide a numerical value\n",
    "- Show your complete calculation\n",
    "- Round to appropriate decimal places if needed\n",
    "- Format your final answer in \\\\boxed{} (e.g., \\\\boxed{2.5} or \\\\boxed{42})\n",
    "\n",
    "Your response should end with your final answer in the format \\\\boxed{X} where X is the numerical answer.\"\"\"\n",
    "        else:\n",
    "            # Default case\n",
    "            prompt += \"\"\"Please analyze the question carefully, reason step-by-step and provide your answer.\n",
    "Show your complete reasoning and format your final answer in \\\\boxed{} (e.g., \\\\boxed{A} for MCQ or \\\\boxed{42} for numerical)\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def extract_answer(self, response_text: str, question_type: str) -> str:\n",
    "        \"\"\"Extract the final answer from model response\"\"\"\n",
    "        try:\n",
    "            # First try to find boxed answer\n",
    "            boxed_match = re.search(r'\\\\boxed\\{([^}]+)\\}', response_text)\n",
    "            if boxed_match:\n",
    "                answer = boxed_match.group(1).strip()\n",
    "            else:\n",
    "                # Try other patterns\n",
    "                answer_patterns = [\n",
    "                    r'\\*\\*Answer:\\*\\*\\s*(.+)',\n",
    "                    r'Answer:\\s*(.+)',\n",
    "                    r'Final answer:\\s*(.+)',\n",
    "                    r'The answer is:\\s*(.+)',\n",
    "                    r'Therefore,?\\s*(.+)',\n",
    "                ]\n",
    "                answer = None\n",
    "                for pattern in answer_patterns:\n",
    "                    match = re.search(pattern, response_text, re.IGNORECASE)\n",
    "                    if match:\n",
    "                        answer = match.group(1).strip()\n",
    "                        break\n",
    "                \n",
    "                if not answer:\n",
    "                    # Take last line as fallback\n",
    "                    lines = response_text.strip().split('\\n')\n",
    "                    answer = lines[-1].strip()\n",
    "            \n",
    "            # Clean up answer based on question type\n",
    "            if question_type == \"MCQ\":\n",
    "                # Extract single letter\n",
    "                match = re.search(r'[ABCD]', answer.upper())\n",
    "                return match.group(0) if match else answer[:10]\n",
    "            elif question_type == \"MCQ(multiple)\":\n",
    "                # Extract multiple letters\n",
    "                letters = re.findall(r'[ABCD]', answer.upper())\n",
    "                unique_letters = sorted(set(letters))\n",
    "                return ''.join(unique_letters) if unique_letters else answer[:20]\n",
    "            elif question_type == \"Integer\":\n",
    "                # Extract number\n",
    "                number_match = re.search(r'-?\\d+\\.?\\d*', answer)\n",
    "                return number_match.group(0) if number_match else answer[:20]\n",
    "            \n",
    "            return answer[:50]\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error extracting answer: {e}\")\n",
    "            return response_text[:50]\n",
    "    \n",
    "    def is_answer_correct(self, predicted_answer: str, question_data: pd.Series) -> bool:\n",
    "        \"\"\"Check if predicted answer is correct\"\"\"\n",
    "        try:\n",
    "            predicted = str(predicted_answer).strip().upper()\n",
    "            correct = str(question_data['gold']).strip().upper()\n",
    "            question_type = question_data['type']\n",
    "            \n",
    "            if question_type == \"MCQ\":\n",
    "                return predicted == correct\n",
    "            elif question_type == \"MCQ(multiple)\":\n",
    "                # Handle multiple choice with multiple correct answers\n",
    "                pred_letters = set(re.findall(r'[ABCD]', predicted))\n",
    "                correct_letters = set(re.findall(r'[ABCD]', correct))\n",
    "                return pred_letters == correct_letters\n",
    "            elif question_type == \"Integer\":\n",
    "                # First try exact match\n",
    "                if predicted == correct:\n",
    "                    return True\n",
    "                # Try numerical comparison with tolerance\n",
    "                try:\n",
    "                    pred_num = float(predicted)\n",
    "                    correct_num = float(correct)\n",
    "                    tolerance = abs(correct_num) * 0.01 if abs(correct_num) > 1 else 0.01\n",
    "                    return abs(pred_num - correct_num) <= tolerance\n",
    "                except ValueError:\n",
    "                    return predicted == correct\n",
    "            \n",
    "            # Default exact match\n",
    "            return predicted == correct\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error comparing answers: {e}\")\n",
    "            return False\n",
    "    \n",
    "    def evaluate_single_question(self, question_data: pd.Series, run_id: int, question_idx: int) -> Optional[Dict]:\n",
    "        \"\"\"Evaluate a single question using InternVL3\"\"\"\n",
    "        try:\n",
    "            prompt = self.create_question_prompt(question_data)\n",
    "            \n",
    "            start_time = time.time()\n",
    "            response_text = self.client.generate_content(prompt)\n",
    "            inference_time = time.time() - start_time\n",
    "            \n",
    "            if not response_text:\n",
    "                logger.error(f\"âŒ Failed to get response for question {question_data.get('index', question_idx)}\")\n",
    "                return None\n",
    "            \n",
    "            predicted_answer = self.extract_answer(response_text, question_data['type'])\n",
    "            is_correct = self.is_answer_correct(predicted_answer, question_data)\n",
    "            \n",
    "            # Log each completion\n",
    "            status = \"âœ… CORRECT\" if is_correct else \"âŒ WRONG\"\n",
    "            logger.info(f\"Run {run_id} | Q{question_idx+1}/{len(self.df)}: {status} ({inference_time:.1f}s) | Predicted: {predicted_answer} | Gold: {question_data['gold']}\")\n",
    "            \n",
    "            return {\n",
    "                'run_id': run_id,\n",
    "                'question_idx': question_idx,\n",
    "                'dataset_index': question_data.get('index', question_idx),\n",
    "                'subject': question_data['subject'],\n",
    "                'question_type': question_data['type'],\n",
    "                'question_text': question_data['question'][:200] + \"...\" if len(question_data['question']) > 200 else question_data['question'],\n",
    "                'correct_answer': question_data['gold'],\n",
    "                'predicted_answer': predicted_answer,\n",
    "                'is_correct': is_correct,\n",
    "                'inference_time': inference_time,\n",
    "                'full_response': response_text[:1000] + \"...\" if len(response_text) > 1000 else response_text  # Truncate for storage\n",
    "            }\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error evaluating question {question_data.get('index', question_idx)}: {e}\")\n",
    "            return None\n",
    "    \n",
    "    def run_single_evaluation_run(self, run_id: int) -> Optional[Dict]:\n",
    "        \"\"\"Run a single evaluation run (sequential processing) with proper resume support\"\"\"\n",
    "        logger.info(f\"\\n{'='*60}\")\n",
    "        logger.info(f\"ðŸš€ Starting Run {run_id}/{self.num_runs}\")\n",
    "        logger.info(f\"{'='*60}\")\n",
    "        \n",
    "        # Check if we're resuming this run\n",
    "        is_resuming_run = (run_id == self.state.current_run and \n",
    "                          self.state.current_run_question_idx > 0)\n",
    "        \n",
    "        if is_resuming_run:\n",
    "            # Resume from existing state\n",
    "            logger.info(f\"ðŸ”„ Resuming Run {run_id} from question {self.state.current_run_question_idx + 1}\")\n",
    "            results = self.state.current_run_results.copy()\n",
    "            shuffled_indices = self.state.current_run_shuffled_indices\n",
    "            start_idx = self.state.current_run_question_idx\n",
    "        else:\n",
    "            # Start new run\n",
    "            logger.info(f\"ðŸ†• Starting new Run {run_id}\")\n",
    "            # Shuffle questions for this run\n",
    "            shuffled_indices = list(range(len(self.df)))\n",
    "            random.seed(run_id)  # Consistent shuffle for each run\n",
    "            random.shuffle(shuffled_indices)\n",
    "            \n",
    "            # Update state for new run\n",
    "            self.state.current_run = run_id\n",
    "            self.state.current_run_question_idx = 0\n",
    "            self.state.current_run_results = []\n",
    "            self.state.current_run_shuffled_indices = shuffled_indices\n",
    "            \n",
    "            results = []\n",
    "            start_idx = 0\n",
    "        \n",
    "        run_start_time = time.time()\n",
    "        \n",
    "        # Process questions sequentially starting from the resume point\n",
    "        for idx in range(start_idx, len(shuffled_indices)):\n",
    "            if self.stop_requested:\n",
    "                logger.info(\"â¹ï¸ Stop requested, ending run early\")\n",
    "                break\n",
    "            \n",
    "            # Get the actual question data using shuffled index\n",
    "            question_data = self.df.iloc[shuffled_indices[idx]]\n",
    "            \n",
    "            result = self.evaluate_single_question(question_data, run_id, idx)\n",
    "            if result:\n",
    "                results.append(result)\n",
    "                \n",
    "                # Update state immediately\n",
    "                self.state.current_run_results = results.copy()\n",
    "                self.state.current_run_question_idx = idx\n",
    "                self.state.completed_questions += 1\n",
    "            \n",
    "            # Save state every 5 questions for minimal data loss\n",
    "            if (idx + 1) % 5 == 0:\n",
    "                self.save_state()\n",
    "                progress = ((idx + 1) / len(shuffled_indices)) * 100\n",
    "                logger.info(f\"ðŸ“Š Run {run_id} Progress: {progress:.1f}% ({idx + 1}/{len(shuffled_indices)} questions) [State Saved]\")\n",
    "            \n",
    "            # Also save partial results every 25 questions\n",
    "            if (idx + 1) % 25 == 0:\n",
    "                self._save_partial_run_results(run_id, results, idx + 1)\n",
    "        \n",
    "        run_duration = time.time() - run_start_time\n",
    "        \n",
    "        if not results:\n",
    "            logger.error(f\"âŒ No valid results for run {run_id}\")\n",
    "            return None\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        correct_count = sum(1 for r in results if r['is_correct'])\n",
    "        accuracy = (correct_count / len(results)) * 100\n",
    "        \n",
    "        # Create run summary\n",
    "        run_summary = {\n",
    "            'run_id': run_id,\n",
    "            'model_name': self.model_name,\n",
    "            'total_questions': len(results),\n",
    "            'correct_answers': correct_count,\n",
    "            'accuracy': accuracy,\n",
    "            'duration': run_duration,\n",
    "            'avg_time_per_question': run_duration / len(results),\n",
    "            'timestamp': datetime.now().strftime(\"%Y%m%d_%H%M%S\"),\n",
    "            'results': results\n",
    "        }\n",
    "        \n",
    "        # Reset run state since this run is complete\n",
    "        self.state.current_run_question_idx = 0\n",
    "        self.state.current_run_results = []\n",
    "        self.state.current_run_shuffled_indices = []\n",
    "        \n",
    "        logger.info(f\"âœ… Run {run_id} completed: {accuracy:.2f}% accuracy ({correct_count}/{len(results)}) in {run_duration:.1f}s\")\n",
    "        logger.info(f\"â±ï¸ Average time per question: {run_summary['avg_time_per_question']:.1f}s\")\n",
    "        \n",
    "        return run_summary\n",
    "    \n",
    "    def save_run_results(self, run_summary: Dict):\n",
    "        \"\"\"Save results for a single run to completed_runs directory\"\"\"\n",
    "        timestamp = run_summary['timestamp']\n",
    "        filename = f\"jeebench_internvl3_run_{run_summary['run_id']:02d}_{timestamp}.json\"\n",
    "        filepath = self.results_dir / \"completed_runs\" / filename\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            json.dump(run_summary, f, indent=2, ensure_ascii=False)\n",
    "        \n",
    "        logger.info(f\"ðŸ’¾ Run {run_summary['run_id']} results saved to: completed_runs/{filename}\")\n",
    "        \n",
    "        # Clean up partial results for this run\n",
    "        self._cleanup_partial_results(run_summary['run_id'])\n",
    "    \n",
    "    def _cleanup_partial_results(self, run_id: int):\n",
    "        \"\"\"Clean up partial result files after a run is completed\"\"\"\n",
    "        partial_dir = self.results_dir / \"partial_results\"\n",
    "        if partial_dir.exists():\n",
    "            # Find and remove partial files for this run\n",
    "            for partial_file in partial_dir.glob(f\"*_run_{run_id:02d}_partial_*\"):\n",
    "                try:\n",
    "                    partial_file.unlink()\n",
    "                    logger.debug(f\"ðŸ—‘ï¸ Cleaned up partial file: {partial_file.name}\")\n",
    "                except Exception as e:\n",
    "                    logger.warning(f\"âš ï¸ Could not clean up partial file {partial_file}: {e}\")\n",
    "    \n",
    "    def calculate_statistics(self, all_run_summaries: List[Dict]) -> Dict:\n",
    "        \"\"\"Calculate overall statistics across all runs\"\"\"\n",
    "        accuracies = [summary['accuracy'] for summary in all_run_summaries]\n",
    "        \n",
    "        return {\n",
    "            'num_runs': len(accuracies),\n",
    "            'mean_accuracy': np.mean(accuracies),\n",
    "            'std_accuracy': np.std(accuracies, ddof=1) if len(accuracies) > 1 else 0,\n",
    "            'sem_accuracy': stats.sem(accuracies) if len(accuracies) > 1 else 0,\n",
    "            'min_accuracy': np.min(accuracies),\n",
    "            'max_accuracy': np.max(accuracies),\n",
    "            'confidence_interval_95': stats.t.interval(\n",
    "                0.95, len(accuracies) - 1,\n",
    "                loc=np.mean(accuracies),\n",
    "                scale=stats.sem(accuracies)\n",
    "            ) if len(accuracies) > 1 else (np.mean(accuracies), np.mean(accuracies)),\n",
    "            'individual_accuracies': accuracies\n",
    "        }\n",
    "    \n",
    "    def analyze_convergence_and_variance(self, all_run_summaries: List[Dict]) -> Dict:\n",
    "        \"\"\"Analyze convergence for optimal k determination\"\"\"\n",
    "        accuracies = [summary['accuracy'] for summary in all_run_summaries]\n",
    "        n_runs = len(accuracies)\n",
    "        \n",
    "        convergence_analysis = {\n",
    "            'k_values': [],\n",
    "            'running_means': [],\n",
    "            'running_stds': [],\n",
    "            'running_sems': [],\n",
    "            'confidence_intervals': [],\n",
    "            'relative_changes': [],\n",
    "            'stability_metrics': [],\n",
    "            'cost_effectiveness': []\n",
    "        }\n",
    "        \n",
    "        for k in range(3, n_runs + 1):\n",
    "            subset_accuracies = accuracies[:k]\n",
    "            \n",
    "            mean_acc = np.mean(subset_accuracies)\n",
    "            std_acc = np.std(subset_accuracies, ddof=1) if k > 1 else 0\n",
    "            sem_acc = stats.sem(subset_accuracies) if k > 1 else 0\n",
    "            \n",
    "            if k > 1:\n",
    "                ci = stats.t.interval(0.95, k-1, loc=mean_acc, scale=sem_acc) if sem_acc > 0 else (mean_acc, mean_acc)\n",
    "                ci_width = ci[1] - ci[0]\n",
    "            else:\n",
    "                ci = (mean_acc, mean_acc)\n",
    "                ci_width = 0\n",
    "            \n",
    "            if k > 3:\n",
    "                prev_mean = convergence_analysis['running_means'][-1]\n",
    "                relative_change = abs(mean_acc - prev_mean) / prev_mean * 100 if prev_mean > 0 else 0\n",
    "            else:\n",
    "                relative_change = np.inf\n",
    "            \n",
    "            cv = std_acc / mean_acc * 100 if mean_acc > 0 else np.inf\n",
    "            cost_effectiveness = ci_width * k\n",
    "            \n",
    "            convergence_analysis['k_values'].append(k)\n",
    "            convergence_analysis['running_means'].append(mean_acc)\n",
    "            convergence_analysis['running_stds'].append(std_acc)\n",
    "            convergence_analysis['running_sems'].append(sem_acc)\n",
    "            convergence_analysis['confidence_intervals'].append(ci)\n",
    "            convergence_analysis['relative_changes'].append(relative_change)\n",
    "            convergence_analysis['stability_metrics'].append(cv)\n",
    "            convergence_analysis['cost_effectiveness'].append(cost_effectiveness)\n",
    "        \n",
    "        return convergence_analysis\n",
    "    \n",
    "    def run_evaluation(self):\n",
    "        \"\"\"Run complete evaluation with resume capability\"\"\"\n",
    "        logger.info(\"ðŸŽ¯ Starting JEEBench InternVL3 8B Evaluation with Resume Capability\")\n",
    "        logger.info(f\"ðŸ“š Dataset: {len(self.df)} questions\")\n",
    "        logger.info(f\"ðŸ¤– Model: {self.model_name}\")\n",
    "        logger.info(f\"ðŸ”„ Total runs planned: {self.num_runs}\")\n",
    "        \n",
    "        try:\n",
    "            # Resume from where we left off\n",
    "            for run_id in range(self.state.current_run, self.num_runs + 1):\n",
    "                if self.stop_requested:\n",
    "                    break\n",
    "                \n",
    "                # Run single evaluation\n",
    "                run_summary = self.run_single_evaluation_run(run_id)\n",
    "                \n",
    "                if run_summary:\n",
    "                    # Save run results\n",
    "                    self.save_run_results(run_summary)\n",
    "                    \n",
    "                    # Update state\n",
    "                    self.state.all_run_summaries.append(run_summary)\n",
    "                    self.state.current_run = run_id + 1\n",
    "                    \n",
    "                    # Save state after each run\n",
    "                    self.save_state()\n",
    "                    \n",
    "                    # Print progress\n",
    "                    progress = (run_id / self.num_runs) * 100\n",
    "                    elapsed = time.time() - self.state.start_time\n",
    "                    eta = (elapsed / run_id) * (self.num_runs - run_id) if run_id > 0 else 0\n",
    "                    \n",
    "                    if len(self.state.all_run_summaries) > 0:\n",
    "                        avg_accuracy = np.mean([s['accuracy'] for s in self.state.all_run_summaries])\n",
    "                        logger.info(f\"ðŸ“Š Progress: {progress:.1f}% | ETA: {eta/3600:.1f}h | Avg accuracy so far: {avg_accuracy:.2f}%\")\n",
    "        \n",
    "        except KeyboardInterrupt:\n",
    "            logger.info(\"â¹ï¸ Evaluation interrupted by user\")\n",
    "            self.interrupted = True\n",
    "        except Exception as e:\n",
    "            logger.error(f\"âŒ Error during evaluation: {e}\")\n",
    "        finally:\n",
    "            # Always save final state\n",
    "            self.save_state()\n",
    "            \n",
    "            # Generate final report if we have results\n",
    "            if self.state.all_run_summaries:\n",
    "                self.generate_final_report()\n",
    "        \n",
    "        logger.info(\"ðŸ Evaluation session ended!\")\n",
    "    \n",
    "    def generate_final_report(self):\n",
    "        \"\"\"Generate comprehensive final report\"\"\"\n",
    "        timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "        report_file = self.results_dir / f\"jeebench_internvl3_final_report_{timestamp}.txt\"\n",
    "        \n",
    "        stats = self.calculate_statistics(self.state.all_run_summaries)\n",
    "        convergence_data = self.analyze_convergence_and_variance(self.state.all_run_summaries)\n",
    "        \n",
    "        # Collect all results for detailed analysis\n",
    "        all_results = []\n",
    "        for run_summary in self.state.all_run_summaries:\n",
    "            all_results.extend(run_summary['results'])\n",
    "        \n",
    "        report_content = f\"\"\"JEEBench InternVL3 8B Evaluation Report\n",
    "{'='*80}\n",
    "\n",
    "Evaluation Date: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n",
    "Model: {self.model_name}\n",
    "Dataset: JEEBench ({len(self.df)} questions)\n",
    "Completed Runs: {len(self.state.all_run_summaries)}/{self.num_runs}\n",
    "Evaluation Mode: Sequential (Single GPU)\n",
    "\n",
    "PERFORMANCE SUMMARY\n",
    "{'-'*40}\n",
    "Mean Accuracy: {stats['mean_accuracy']:.2f}% Â± {stats['std_accuracy']:.2f}%\n",
    "Standard Error: {stats['sem_accuracy']:.2f}%\n",
    "95% Confidence Interval: [{stats['confidence_interval_95'][0]:.2f}%, {stats['confidence_interval_95'][1]:.2f}%]\n",
    "Range: {stats['min_accuracy']:.2f}% - {stats['max_accuracy']:.2f}%\n",
    "\n",
    "PERFORMANCE BREAKDOWN\n",
    "{'-'*40}\n",
    "\"\"\"\n",
    "        \n",
    "        # Performance by category\n",
    "        categories = ['subject', 'question_type']\n",
    "        for category in categories:\n",
    "            report_content += f\"\\nBy {category.title()}:\\n\"\n",
    "            category_stats = defaultdict(lambda: {'correct': 0, 'total': 0})\n",
    "            \n",
    "            for result in all_results:\n",
    "                cat_value = result[category]\n",
    "                category_stats[cat_value]['total'] += 1\n",
    "                if result['is_correct']:\n",
    "                    category_stats[cat_value]['correct'] += 1\n",
    "            \n",
    "            for cat_value, stats_dict in sorted(category_stats.items()):\n",
    "                accuracy = (stats_dict['correct'] / stats_dict['total']) * 100\n",
    "                report_content += f\"  {cat_value}: {accuracy:.2f}% ({stats_dict['correct']}/{stats_dict['total']})\\n\"\n",
    "        \n",
    "        # Add individual run accuracies\n",
    "        report_content += f\"\\nINDIVIDUAL RUN ACCURACIES\\n{'-'*40}\\n\"\n",
    "        for i, accuracy in enumerate(stats['individual_accuracies'], 1):\n",
    "            report_content += f\"Run {i}: {accuracy:.2f}%\\n\"\n",
    "        \n",
    "        # Add timing information\n",
    "        if all_results:\n",
    "            avg_inference_time = np.mean([r['inference_time'] for r in all_results])\n",
    "            total_inference_time = sum([r['inference_time'] for r in all_results])\n",
    "            report_content += f\"\\nTIMING ANALYSIS\\n{'-'*40}\\n\"\n",
    "            report_content += f\"Average inference time per question: {avg_inference_time:.2f}s\\n\"\n",
    "            report_content += f\"Total inference time: {total_inference_time/3600:.2f}h\\n\"\n",
    "        \n",
    "        with open(report_file, 'w', encoding='utf-8') as f:\n",
    "            f.write(report_content)\n",
    "        \n",
    "        logger.info(f\"ðŸ“„ Final report saved to: {report_file}\")\n",
    "        logger.info(f\"ðŸŽ¯ Final Pass@1 Accuracy: {stats['mean_accuracy']:.2f}% Â± {stats['std_accuracy']:.2f}%\")\n",
    "\n",
    "# Configuration for Jupyter Notebook\n",
    "\n",
    "# Model configuration\n",
    "MODEL_NAME = \"internvl3-8b-instruct\"  # Change this if your model has a different name in LM Studio\n",
    "NUM_RUNS = 10\n",
    "\n",
    "def run_evaluation():\n",
    "    \"\"\"Main evaluation function for Jupyter\"\"\"\n",
    "    evaluator = JEEBenchInternVL3Evaluator(NUM_RUNS, MODEL_NAME)\n",
    "    evaluator.run_evaluation()\n",
    "\n",
    "def resume_evaluation():\n",
    "    \"\"\"Resume evaluation from saved state\"\"\"\n",
    "    evaluator = JEEBenchInternVL3Evaluator(NUM_RUNS, MODEL_NAME)\n",
    "    logger.info(\"ðŸ”„ Resuming evaluation from saved state...\")\n",
    "    evaluator.run_evaluation()\n",
    "\n",
    "def check_progress():\n",
    "    \"\"\"Check current progress without running evaluation\"\"\"\n",
    "    evaluator = JEEBenchInternVL3Evaluator(NUM_RUNS, MODEL_NAME)\n",
    "    state = evaluator.state\n",
    "    \n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"CURRENT EVALUATION PROGRESS\")\n",
    "    print(f\"{'='*60}\")\n",
    "    print(f\"Current Run: {state.current_run}/{NUM_RUNS}\")\n",
    "    \n",
    "    if state.current_run_question_idx > 0:\n",
    "        print(f\"Current Run Progress: Question {state.current_run_question_idx + 1}/{len(evaluator.df)} within Run {state.current_run}\")\n",
    "        current_run_progress = ((state.current_run_question_idx + 1) / len(evaluator.df)) * 100\n",
    "        print(f\"Current Run Progress: {current_run_progress:.1f}%\")\n",
    "    \n",
    "    print(f\"Total Completed Questions: {state.completed_questions:,}/{state.total_questions:,}\")\n",
    "    print(f\"Overall Progress: {(state.completed_questions/state.total_questions)*100:.1f}%\")\n",
    "    \n",
    "    if state.all_run_summaries:\n",
    "        accuracies = [s['accuracy'] for s in state.all_run_summaries]\n",
    "        print(f\"Completed Runs: {len(state.all_run_summaries)}\")\n",
    "        print(f\"Average Accuracy: {np.mean(accuracies):.2f}% Â± {np.std(accuracies, ddof=1):.2f}%\")\n",
    "        \n",
    "        elapsed = time.time() - state.start_time\n",
    "        if len(state.all_run_summaries) > 0:\n",
    "            eta = (elapsed / len(state.all_run_summaries)) * (NUM_RUNS - len(state.all_run_summaries))\n",
    "            print(f\"Elapsed Time: {elapsed/3600:.1f}h\")\n",
    "            print(f\"Estimated Time Remaining: {eta/3600:.1f}h\")\n",
    "    \n",
    "    # Show current run results if any\n",
    "    if state.current_run_results:\n",
    "        current_correct = sum(1 for r in state.current_run_results if r['is_correct'])\n",
    "        current_accuracy = (current_correct / len(state.current_run_results)) * 100\n",
    "        print(f\"Current Run {state.current_run} so far: {current_accuracy:.1f}% ({current_correct}/{len(state.current_run_results)})\")\n",
    "    \n",
    "    print(f\"Last Save: {datetime.fromtimestamp(state.last_save_time).strftime('%Y-%m-%d %H:%M:%S')}\")\n",
    "    print(f\"State File: {evaluator.state_file}\")\n",
    "    \n",
    "    # Check for partial results\n",
    "    partial_dir = evaluator.results_dir / \"partial_results\"\n",
    "    if partial_dir.exists():\n",
    "        partial_files = list(partial_dir.glob(\"*.json\"))\n",
    "        if partial_files:\n",
    "            print(f\"Partial Result Files: {len(partial_files)}\")\n",
    "            print(\"(These will be cleaned up when runs complete)\")\n",
    "    \n",
    "    print(f\"{'='*60}\\n\")\n",
    "\n",
    "def recover_from_partial_results():\n",
    "    \"\"\"Attempt to recover progress from partial result files if state is corrupted\"\"\"\n",
    "    evaluator = JEEBenchInternVL3Evaluator(NUM_RUNS, MODEL_NAME)\n",
    "    partial_dir = evaluator.results_dir / \"partial_results\"\n",
    "    \n",
    "    if not partial_dir.exists():\n",
    "        print(\"âŒ No partial results directory found\")\n",
    "        return\n",
    "    \n",
    "    partial_files = list(partial_dir.glob(\"*.json\"))\n",
    "    if not partial_files:\n",
    "        print(\"âŒ No partial result files found\")\n",
    "        return\n",
    "    \n",
    "    print(f\"ðŸ” Found {len(partial_files)} partial result files:\")\n",
    "    for pf in sorted(partial_files):\n",
    "        try:\n",
    "            with open(pf, 'r') as f:\n",
    "                data = json.load(f)\n",
    "            print(f\"  ðŸ“„ {pf.name}: Run {data.get('run_id', '?')}, {data.get('questions_completed', '?')} questions, {data.get('partial_accuracy', '?'):.1f}% accuracy\")\n",
    "        except Exception as e:\n",
    "            print(f\"  âŒ {pf.name}: Error reading file - {e}\")\n",
    "    \n",
    "    print(\"\\nðŸ’¡ You can manually review these files if needed for data recovery\")\n",
    "    print(\"ðŸ”„ Try running resume_evaluation() to continue from the last saved state\")\n",
    "\n",
    "def reset_evaluation():\n",
    "    \"\"\"Reset evaluation state (use with caution!)\"\"\"\n",
    "    evaluator = JEEBenchInternVL3Evaluator(NUM_RUNS, MODEL_NAME)\n",
    "    \n",
    "    print(\"âš ï¸  WARNING: This will delete all progress and start fresh!\")\n",
    "    confirm = input(\"Type 'RESET' to confirm: \")\n",
    "    \n",
    "    if confirm == \"RESET\":\n",
    "        files_to_remove = []\n",
    "        \n",
    "        # Main state file\n",
    "        if evaluator.state_file.exists():\n",
    "            files_to_remove.append(evaluator.state_file)\n",
    "        \n",
    "        # Backup state file\n",
    "        backup_file = evaluator.state_file.with_suffix('.backup')\n",
    "        if backup_file.exists():\n",
    "            files_to_remove.append(backup_file)\n",
    "        \n",
    "        # Remove all files\n",
    "        for file_path in files_to_remove:\n",
    "            try:\n",
    "                file_path.unlink()\n",
    "                print(f\"âœ… Removed: {file_path.name}\")\n",
    "            except Exception as e:\n",
    "                print(f\"âŒ Error removing {file_path.name}: {e}\")\n",
    "        \n",
    "        if files_to_remove:\n",
    "            print(\"âœ… Evaluation state reset successfully!\")\n",
    "        else:\n",
    "            print(\"â„¹ï¸  No existing state files found.\")\n",
    "    else:\n",
    "        print(\"âŒ Reset cancelled.\")\n",
    "\n",
    "def force_upgrade_state():\n",
    "    \"\"\"Force upgrade of old state file format\"\"\"\n",
    "    try:\n",
    "        evaluator = JEEBenchInternVL3Evaluator(NUM_RUNS, MODEL_NAME)\n",
    "        print(\"âœ… State file upgraded successfully!\")\n",
    "        check_progress()\n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error upgrading state: {e}\")\n",
    "        print(\"ðŸ’¡ Try reset_evaluation() if the issue persists\")\n",
    "\n",
    "def load_jeebench_sample(n_samples: int = 5):\n",
    "    \"\"\"Load and display a sample of JEEBench questions for inspection\"\"\"\n",
    "    from datasets import load_dataset\n",
    "    \n",
    "    print(\"ðŸ“š Loading JEEBench dataset...\")\n",
    "    ds = load_dataset(\"daman1209arora/jeebench\")\n",
    "    df = pd.DataFrame(ds['test'])\n",
    "    \n",
    "    print(f\"\\nðŸ“Š Dataset Info:\")\n",
    "    print(f\"Total questions: {len(df)}\")\n",
    "    print(f\"Columns: {list(df.columns)}\")\n",
    "    print(f\"Question types: {df['type'].value_counts().to_dict()}\")\n",
    "    print(f\"Subjects: {df['subject'].value_counts().to_dict()}\")\n",
    "    \n",
    "    print(f\"\\nðŸ” Sample {n_samples} questions:\")\n",
    "    print(\"=\"*80)\n",
    "    \n",
    "    sample_df = df.sample(n=min(n_samples, len(df)), random_state=42)\n",
    "    \n",
    "    for idx, row in sample_df.iterrows():\n",
    "        print(f\"\\nðŸ“ Question {idx + 1}:\")\n",
    "        print(f\"Subject: {row['subject']}\")\n",
    "        print(f\"Type: {row['type']}\")\n",
    "        print(f\"Question: {row['question'][:200]}{'...' if len(row['question']) > 200 else ''}\")\n",
    "        print(f\"Gold Answer: {row['gold']}\")\n",
    "        print(\"-\" * 60)\n",
    "    \n",
    "    return df\n",
    "\n",
    "def test_model_connection():\n",
    "    \"\"\"Test if InternVL3 model is accessible via LM Studio\"\"\"\n",
    "    try:\n",
    "        print(\"ðŸ”„ Testing InternVL3 model connection...\")\n",
    "        model = lms.llm(MODEL_NAME)\n",
    "        \n",
    "        test_prompt = \"Hello! Can you solve this simple math problem: What is 2 + 2?\"\n",
    "        response = model.respond(test_prompt)\n",
    "        \n",
    "        print(\"âœ… Model connection successful!\")\n",
    "        print(f\"ðŸ“ Test response: {response}\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Model connection failed: {e}\")\n",
    "        print(\"ðŸ”§ Please ensure:\")\n",
    "        print(\"  1. LM Studio is running\")\n",
    "        print(\"  2. InternVL3 8B model is loaded\")\n",
    "        print(\"  3. API server is enabled in LM Studio\")\n",
    "        print(f\"  4. Model name '{MODEL_NAME}' is correct\")\n",
    "        return False\n",
    "\n",
    "# Usage examples for Jupyter:\n",
    "\"\"\"\n",
    "# 1. First, test if the model is accessible:\n",
    "test_model_connection()\n",
    "\n",
    "# 2. To inspect the dataset first:\n",
    "df = load_jeebench_sample(10)\n",
    "\n",
    "# 3. To start new evaluation:\n",
    "run_evaluation()\n",
    "\n",
    "# 4. To resume from where you left off:\n",
    "resume_evaluation()\n",
    "\n",
    "# 5. To check current progress:\n",
    "check_progress()\n",
    "\n",
    "# 6. If you get state compatibility errors, try:\n",
    "force_upgrade_state()\n",
    "\n",
    "# 7. To recover from partial results if state is corrupted:\n",
    "recover_from_partial_results()\n",
    "\n",
    "# 8. To reset everything (use carefully!):\n",
    "reset_evaluation()\n",
    "\n",
    "# Example of running a single question for testing:\n",
    "def test_single_question():\n",
    "    evaluator = JEEBenchInternVL3Evaluator(1, MODEL_NAME)\n",
    "    sample_question = evaluator.df.iloc[0]\n",
    "    result = evaluator.evaluate_single_question(sample_question, 1, 0)\n",
    "    print(\"Test result:\", result)\n",
    "    return result\n",
    "\n",
    "# Note: With the new implementation, state is saved every 5 questions\n",
    "# and partial results are saved every 25 questions, so you'll lose at most\n",
    "# 4 questions of work if the evaluation is interrupted.\n",
    "\n",
    "# If you encounter 'EvaluationState' object has no attribute errors:\n",
    "# This happens when you have an old state file. Try force_upgrade_state()\n",
    "# or reset_evaluation() to start fresh.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bcd3ad2f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ”„ Testing InternVL3 model connection...\n",
      "âœ… Model connection successful!\n",
      "ðŸ“ Test response: Certainly! The sum of \\(2\\) and \\(2\\) is calculated as follows:\n",
      "\n",
      "\\[ 2 + 2 = 4 \\]\n",
      "\n",
      "Therefore, the answer to your question is \\(\\boxed{4}\\).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_model_connection()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45132d24",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-28 03:09:10,200 - INFO - ðŸ“š Loading JEEBench dataset...\n",
      "2025-07-28 03:09:14,268 - INFO - ðŸ“Š Loaded JEEBench dataset with 515 questions\n",
      "2025-07-28 03:09:14,269 - INFO - ðŸ“‹ Dataset columns: ['subject', 'description', 'gold', 'index', 'type', 'question']\n",
      "2025-07-28 03:09:14,270 - INFO - ðŸ“ Question types: {'MCQ(multiple)': 186, 'Numeric': 137, 'MCQ': 110, 'Integer': 82}\n",
      "2025-07-28 03:09:14,271 - INFO - ðŸ“š Subjects: {'math': 236, 'chem': 156, 'phy': 123}\n",
      "2025-07-28 03:09:14,272 - INFO - ðŸš€ Initializing InternVL3 8B model...\n",
      "2025-07-28 03:09:14,274 - INFO - âœ… InternVL3 8B model loaded successfully: internvl3-8b-instruct\n",
      "2025-07-28 03:09:14,850 - INFO - ðŸ“ Model test response: Yes, I can see your message! How can I assist you today? If you have any questions or need help with...\n",
      "2025-07-28 03:09:14,852 - INFO - ðŸ’¾ Results will be saved to: jeebench_internvl3_evaluation_results\n",
      "2025-07-28 03:09:14,853 - INFO - ðŸŽ¯ Running 10 evaluations with InternVL3 8B\n",
      "2025-07-28 03:09:14,853 - INFO - ðŸŽ¯ Starting JEEBench InternVL3 8B Evaluation with Resume Capability\n",
      "2025-07-28 03:09:14,854 - INFO - ðŸ“š Dataset: 515 questions\n",
      "2025-07-28 03:09:14,854 - INFO - ðŸ¤– Model: internvl3-8b-instruct\n",
      "2025-07-28 03:09:14,855 - INFO - ðŸ”„ Total runs planned: 10\n",
      "2025-07-28 03:09:14,855 - INFO - \n",
      "============================================================\n",
      "2025-07-28 03:09:14,856 - INFO - ðŸš€ Starting Run 1/10\n",
      "2025-07-28 03:09:14,856 - INFO - ============================================================\n",
      "2025-07-28 03:09:20,536 - INFO - Run 1 | Q1/515: âŒ WRONG (5.7s) | Predicted: AC | Gold: BC\n",
      "2025-07-28 03:09:29,676 - INFO - Run 1 | Q2/515: âŒ WRONG (9.1s) | Predicted: 0.2 | Gold: 0.16\n",
      "2025-07-28 03:09:38,453 - INFO - Run 1 | Q3/515: âŒ WRONG (8.8s) | Predicted: 700 | Gold: 8.33\n",
      "2025-07-28 03:09:44,088 - INFO - Run 1 | Q4/515: âŒ WRONG (5.6s) | Predicted: 0 | Gold: 0.69\n",
      "2025-07-28 03:09:51,446 - INFO - Run 1 | Q5/515: âŒ WRONG (7.4s) | Predicted: AD | Gold: CD\n",
      "2025-07-28 03:10:05,124 - INFO - Run 1 | Q6/515: âœ… CORRECT (13.7s) | Predicted: 8500 | Gold: 8500\n",
      "2025-07-28 03:10:09,895 - INFO - Run 1 | Q7/515: âŒ WRONG (4.8s) | Predicted: BD | Gold: ABD\n",
      "2025-07-28 03:10:17,663 - INFO - Run 1 | Q8/515: âœ… CORRECT (7.8s) | Predicted: C | Gold: C\n",
      "2025-07-28 03:10:25,986 - INFO - Run 1 | Q9/515: âŒ WRONG (8.3s) | Predicted: \\frac{49 | Gold: 0.5\n",
      "2025-07-28 03:10:33,108 - INFO - Run 1 | Q10/515: âŒ WRONG (7.1s) | Predicted: BCD | Gold: CD\n",
      "2025-07-28 03:10:42,945 - INFO - Run 1 | Q11/515: âŒ WRONG (9.8s) | Predicted: AB | Gold: BC\n",
      "2025-07-28 03:11:04,447 - INFO - Run 1 | Q12/515: âŒ WRONG (21.5s) | Predicted: BD | Gold: ABC\n",
      "2025-07-28 03:11:12,619 - INFO - Run 1 | Q13/515: âŒ WRONG (8.2s) | Predicted: C | Gold: A\n",
      "2025-07-28 03:11:22,779 - INFO - Run 1 | Q14/515: âœ… CORRECT (10.2s) | Predicted: 3 | Gold: 3\n",
      "2025-07-28 03:11:30,321 - INFO - Run 1 | Q15/515: âŒ WRONG (7.5s) | Predicted: 0.05 | Gold: 2.5\n",
      "2025-07-28 03:11:40,911 - INFO - Run 1 | Q16/515: âŒ WRONG (10.6s) | Predicted: A | Gold: BC\n",
      "2025-07-28 03:11:55,728 - INFO - Run 1 | Q17/515: âŒ WRONG (14.8s) | Predicted: -\\infty | Gold: 2\n",
      "2025-07-28 03:11:58,874 - INFO - Run 1 | Q18/515: âœ… CORRECT (3.1s) | Predicted: 4 | Gold: 4\n",
      "2025-07-28 03:12:08,203 - INFO - Run 1 | Q19/515: âœ… CORRECT (9.3s) | Predicted: 76.25 | Gold: 76.25\n",
      "2025-07-28 03:12:19,914 - INFO - Run 1 | Q20/515: âŒ WRONG (11.7s) | Predicted: 1 | Gold: 4\n",
      "2025-07-28 03:12:29,502 - INFO - Run 1 | Q21/515: âœ… CORRECT (9.6s) | Predicted: BCD | Gold: BCD\n",
      "2025-07-28 03:12:36,967 - INFO - Run 1 | Q22/515: âŒ WRONG (7.5s) | Predicted: 0 | Gold: 6\n",
      "2025-07-28 03:12:44,613 - INFO - Run 1 | Q23/515: âœ… CORRECT (7.6s) | Predicted: C | Gold: C\n",
      "2025-07-28 03:12:51,891 - INFO - Run 1 | Q24/515: âŒ WRONG (7.3s) | Predicted: \\frac{\\pi | Gold: 0.5\n",
      "2025-07-28 03:12:58,788 - INFO - Run 1 | Q25/515: âŒ WRONG (6.9s) | Predicted: [L]=4 | Gold: 1\n",
      "2025-07-28 03:13:05,829 - INFO - Run 1 | Q26/515: âœ… CORRECT (7.0s) | Predicted: A | Gold: A\n",
      "2025-07-28 03:13:12,424 - INFO - Run 1 | Q27/515: âœ… CORRECT (6.6s) | Predicted: C | Gold: C\n",
      "2025-07-28 03:13:33,917 - INFO - Run 1 | Q28/515: âœ… CORRECT (21.5s) | Predicted: C | Gold: C\n",
      "2025-07-28 03:13:47,395 - INFO - Run 1 | Q29/515: âœ… CORRECT (13.5s) | Predicted: A | Gold: A\n",
      "2025-07-28 03:13:54,871 - INFO - Run 1 | Q30/515: âœ… CORRECT (7.5s) | Predicted: D | Gold: D\n",
      "2025-07-28 03:14:08,217 - INFO - Run 1 | Q31/515: âŒ WRONG (13.3s) | Predicted: B | Gold: BC\n",
      "2025-07-28 03:14:18,696 - INFO - Run 1 | Q32/515: âŒ WRONG (10.5s) | Predicted: 2962 | Gold: 3780\n",
      "2025-07-28 03:14:34,896 - INFO - Run 1 | Q33/515: âŒ WRONG (16.2s) | Predicted: \\[ g(2) = 0 \\] | Gold: BC\n",
      "2025-07-28 03:14:49,602 - INFO - Run 1 | Q34/515: âŒ WRONG (14.7s) | Predicted: 538.92 + (3.97 \\times \\text{Molar mass of Nickel-A | Gold: 2992\n",
      "2025-07-28 03:15:00,046 - INFO - Run 1 | Q35/515: âŒ WRONG (10.4s) | Predicted: \\frac{1+\\sqrt{3 | Gold: 2\n",
      "2025-07-28 03:15:12,972 - INFO - Run 1 | Q36/515: âŒ WRONG (12.9s) | Predicted: A | Gold: C\n",
      "2025-07-28 03:15:26,066 - INFO - Run 1 | Q37/515: âŒ WRONG (13.1s) | Predicted: 33.6 | Gold: 8\n",
      "2025-07-28 03:15:34,426 - INFO - Run 1 | Q38/515: âœ… CORRECT (8.4s) | Predicted: 19 | Gold: 19\n",
      "2025-07-28 03:15:47,958 - INFO - Run 1 | Q39/515: âŒ WRONG (13.5s) | Predicted: ABD | Gold: ACD\n",
      "2025-07-28 03:15:59,911 - INFO - Run 1 | Q40/515: âŒ WRONG (12.0s) | Predicted: C | Gold: AD\n",
      "2025-07-28 03:16:07,613 - INFO - Run 1 | Q41/515: âŒ WRONG (7.7s) | Predicted: 8 | Gold: 6\n",
      "2025-07-28 03:16:20,511 - INFO - Run 1 | Q42/515: âœ… CORRECT (12.9s) | Predicted: BCD | Gold: BCD\n",
      "2025-07-28 03:16:32,143 - INFO - Run 1 | Q43/515: âŒ WRONG (11.6s) | Predicted: \\frac{2 | Gold: 1.5\n",
      "2025-07-28 03:16:46,627 - INFO - Run 1 | Q44/515: âŒ WRONG (14.5s) | Predicted: A | Gold: AD\n",
      "2025-07-28 03:16:55,541 - INFO - Run 1 | Q45/515: âŒ WRONG (8.9s) | Predicted: CD | Gold: ACD\n",
      "2025-07-28 03:17:04,588 - INFO - Run 1 | Q46/515: âŒ WRONG (9.0s) | Predicted: B | Gold: A\n",
      "2025-07-28 03:17:22,470 - INFO - Run 1 | Q47/515: âŒ WRONG (17.9s) | Predicted: AB | Gold: ABC\n",
      "2025-07-28 03:17:32,260 - INFO - Run 1 | Q48/515: âŒ WRONG (9.8s) | Predicted: 15.2 | Gold: 31\n",
      "2025-07-28 03:17:48,981 - INFO - Run 1 | Q49/515: âŒ WRONG (16.7s) | Predicted: ABCD | Gold: ACD\n",
      "2025-07-28 03:17:59,018 - INFO - Run 1 | Q50/515: âŒ WRONG (10.0s) | Predicted: B | Gold: A\n",
      "2025-07-28 03:17:59,020 - INFO - ðŸ’¾ State saved: Run 1, 50/5150 questions\n",
      "2025-07-28 03:17:59,020 - INFO - ðŸ“Š Run 1 Progress: 9.7% (50/515 questions)\n",
      "2025-07-28 03:18:14,005 - INFO - Run 1 | Q51/515: âœ… CORRECT (15.0s) | Predicted: AB | Gold: AB\n",
      "2025-07-28 03:18:30,489 - INFO - Run 1 | Q52/515: âŒ WRONG (16.5s) | Predicted: BD | Gold: ABC\n",
      "2025-07-28 03:18:36,701 - INFO - Run 1 | Q53/515: âœ… CORRECT (6.2s) | Predicted: A | Gold: A\n",
      "2025-07-28 03:18:45,819 - INFO - Run 1 | Q54/515: âœ… CORRECT (9.1s) | Predicted: BC | Gold: BC\n",
      "2025-07-28 07:38:46,320 - INFO - Run 1 | Q55/515: âŒ WRONG (15600.5s) | Predicted: [M L^{-1} I^{-2} T^{ | Gold: BD\n",
      "2025-07-28 07:38:56,422 - INFO - Run 1 | Q56/515: âŒ WRONG (10.1s) | Predicted: ABCD | Gold: ABC\n",
      "2025-07-28 07:39:16,384 - INFO - Run 1 | Q57/515: âŒ WRONG (20.0s) | Predicted: D | Gold: ABD\n",
      "2025-07-28 07:39:30,376 - INFO - Run 1 | Q58/515: âŒ WRONG (14.0s) | Predicted: A | Gold: AC\n",
      "2025-07-28 07:40:30,283 - INFO - Run 1 | Q59/515: âŒ WRONG (59.9s) | Predicted: : | Gold: BD\n",
      "2025-07-28 07:40:38,764 - INFO - Run 1 | Q60/515: âŒ WRONG (8.5s) | Predicted: A | Gold: ABD\n",
      "2025-07-28 07:40:57,953 - INFO - Run 1 | Q61/515: âŒ WRONG (19.2s) | Predicted: C | Gold: A\n",
      "2025-07-28 07:41:04,454 - INFO - Run 1 | Q62/515: âŒ WRONG (6.5s) | Predicted: A | Gold: BD\n",
      "2025-07-28 07:41:13,731 - INFO - Run 1 | Q63/515: âŒ WRONG (9.3s) | Predicted: \\frac{1 | Gold: 0.5\n",
      "2025-07-28 07:41:22,460 - INFO - Run 1 | Q64/515: âŒ WRONG (8.7s) | Predicted: \\frac{3\\pi | Gold: 0.5\n",
      "2025-07-28 07:41:37,761 - INFO - Run 1 | Q65/515: âŒ WRONG (15.3s) | Predicted: B | Gold: C\n",
      "2025-07-28 07:41:48,300 - INFO - Run 1 | Q66/515: âŒ WRONG (10.5s) | Predicted: ABC | Gold: AB\n",
      "2025-07-28 07:42:01,686 - INFO - Run 1 | Q67/515: âœ… CORRECT (13.4s) | Predicted: A | Gold: A\n",
      "2025-07-28 07:42:14,625 - INFO - Run 1 | Q68/515: âŒ WRONG (12.9s) | Predicted: AC | Gold: AB\n",
      "2025-07-28 07:42:25,811 - INFO - Run 1 | Q69/515: âŒ WRONG (11.2s) | Predicted: A | Gold: BD\n",
      "2025-07-28 07:42:31,825 - INFO - Run 1 | Q70/515: âŒ WRONG (6.0s) | Predicted: ABD | Gold: ABCD\n",
      "2025-07-28 07:42:40,129 - INFO - Run 1 | Q71/515: âŒ WRONG (8.3s) | Predicted: 0.138 | Gold: 6.47\n",
      "2025-07-28 07:42:52,776 - INFO - Run 1 | Q72/515: âŒ WRONG (12.6s) | Predicted: BC | Gold: ABD\n",
      "2025-07-28 07:43:04,758 - INFO - Run 1 | Q73/515: âŒ WRONG (12.0s) | Predicted: \\frac{3 | Gold: 1\n",
      "2025-07-28 07:43:29,295 - INFO - Run 1 | Q74/515: âŒ WRONG (24.5s) | Predicted: 5 | Gold: 8\n",
      "2025-07-28 07:43:52,565 - INFO - Run 1 | Q75/515: âŒ WRONG (23.3s) | Predicted: A | Gold: BCD\n",
      "2025-07-28 07:44:01,991 - INFO - Run 1 | Q76/515: âŒ WRONG (9.4s) | Predicted: 1.60 | Gold: 0.32\n",
      "2025-07-28 07:44:15,509 - INFO - Run 1 | Q77/515: âœ… CORRECT (13.5s) | Predicted: B | Gold: B\n",
      "2025-07-28 07:44:28,360 - INFO - Run 1 | Q78/515: âŒ WRONG (12.9s) | Predicted: : | Gold: BC\n",
      "2025-07-28 07:44:33,768 - INFO - Run 1 | Q79/515: âŒ WRONG (5.4s) | Predicted: BD | Gold: BC\n",
      "2025-07-28 07:44:50,361 - INFO - Run 1 | Q80/515: âŒ WRONG (16.6s) | Predicted: 1.69 \\times 10^{10 | Gold: 24\n",
      "2025-07-28 07:44:58,775 - INFO - Run 1 | Q81/515: âŒ WRONG (8.4s) | Predicted: AC | Gold: AD\n",
      "2025-07-28 07:45:05,301 - INFO - Run 1 | Q82/515: âŒ WRONG (6.5s) | Predicted: 45 | Gold: 5\n",
      "2025-07-28 07:45:12,800 - INFO - Run 1 | Q83/515: âŒ WRONG (7.5s) | Predicted: 0.2 | Gold: 0.18\n",
      "2025-07-28 07:45:19,929 - INFO - Run 1 | Q84/515: âŒ WRONG (7.1s) | Predicted: B | Gold: D\n",
      "2025-07-28 07:45:35,139 - INFO - Run 1 | Q85/515: âœ… CORRECT (15.2s) | Predicted: AC | Gold: AC\n",
      "2025-07-28 07:45:43,334 - INFO - Run 1 | Q86/515: âœ… CORRECT (8.2s) | Predicted: BCD | Gold: BCD\n",
      "2025-07-28 07:45:50,699 - INFO - Run 1 | Q87/515: âŒ WRONG (7.4s) | Predicted: ABCD | Gold: ACD\n",
      "2025-07-28 07:45:58,828 - INFO - Run 1 | Q88/515: âœ… CORRECT (8.1s) | Predicted: B | Gold: B\n",
      "2025-07-28 07:46:07,033 - INFO - Run 1 | Q89/515: âŒ WRONG (8.2s) | Predicted: ACD | Gold: AB\n",
      "2025-07-28 07:46:24,588 - INFO - Run 1 | Q90/515: âŒ WRONG (17.6s) | Predicted: CD | Gold: ACD\n",
      "2025-07-28 07:46:41,624 - INFO - Run 1 | Q91/515: âŒ WRONG (17.0s) | Predicted: 10^{-10.5 | Gold: 0.2\n",
      "2025-07-28 07:46:50,862 - INFO - Run 1 | Q92/515: âŒ WRONG (9.2s) | Predicted: \\alpha - 2\\beta + \\gamma | Gold: 1.00\n",
      "2025-07-28 07:47:01,167 - INFO - Run 1 | Q93/515: âŒ WRONG (10.3s) | Predicted: 4.09 | Gold: 1.2\n",
      "2025-07-28 07:47:12,431 - INFO - Run 1 | Q94/515: âŒ WRONG (11.3s) | Predicted: A | Gold: C\n",
      "2025-07-28 07:47:25,340 - INFO - Run 1 | Q95/515: âŒ WRONG (12.9s) | Predicted: AB | Gold: ABD\n",
      "2025-07-28 07:47:36,053 - INFO - Run 1 | Q96/515: âŒ WRONG (10.7s) | Predicted: -2 | Gold: 4\n",
      "2025-07-28 07:47:47,747 - INFO - Run 1 | Q97/515: âŒ WRONG (11.7s) | Predicted: 1.15 | Gold: 8\n",
      "2025-07-28 07:47:57,540 - INFO - Run 1 | Q98/515: âœ… CORRECT (9.8s) | Predicted: 3 | Gold: 3\n",
      "2025-07-28 07:48:04,123 - INFO - Run 1 | Q99/515: âœ… CORRECT (6.6s) | Predicted: 5 | Gold: 5\n",
      "2025-07-28 07:48:12,500 - INFO - Run 1 | Q100/515: âŒ WRONG (8.4s) | Predicted: ABD | Gold: AB\n",
      "2025-07-28 07:48:12,502 - INFO - ðŸ’¾ State saved: Run 1, 100/5150 questions\n",
      "2025-07-28 07:48:12,503 - INFO - ðŸ“Š Run 1 Progress: 19.4% (100/515 questions)\n",
      "2025-07-28 07:48:27,355 - INFO - Run 1 | Q101/515: âŒ WRONG (14.9s) | Predicted: 33 | Gold: 1\n",
      "2025-07-28 07:48:36,069 - INFO - Run 1 | Q102/515: âœ… CORRECT (8.7s) | Predicted: 24.5 | Gold: 24.5\n",
      "2025-07-28 07:48:40,331 - INFO - Run 1 | Q103/515: âœ… CORRECT (4.3s) | Predicted: A | Gold: A\n",
      "2025-07-28 07:49:01,241 - INFO - Run 1 | Q104/515: âŒ WRONG (20.9s) | Predicted: B | Gold: C\n",
      "2025-07-28 07:49:13,514 - INFO - Run 1 | Q105/515: âŒ WRONG (12.3s) | Predicted: D | Gold: B\n",
      "2025-07-28 07:49:36,125 - INFO - Run 1 | Q106/515: âŒ WRONG (22.6s) | Predicted: ABCD | Gold: AC\n",
      "2025-07-28 07:49:44,952 - INFO - Run 1 | Q107/515: âŒ WRONG (8.8s) | Predicted: D | Gold: BCD\n",
      "2025-07-28 07:49:54,304 - INFO - Run 1 | Q108/515: âŒ WRONG (9.4s) | Predicted: 3 | Gold: 5\n",
      "2025-07-28 07:50:04,336 - INFO - Run 1 | Q109/515: âŒ WRONG (10.0s) | Predicted: 916 | Gold: 646\n",
      "2025-07-28 07:50:16,769 - INFO - Run 1 | Q110/515: âŒ WRONG (12.4s) | Predicted: C | Gold: B\n",
      "2025-07-28 07:50:22,405 - INFO - Run 1 | Q111/515: âœ… CORRECT (5.6s) | Predicted: C | Gold: C\n",
      "2025-07-28 07:50:28,407 - INFO - Run 1 | Q112/515: âŒ WRONG (6.0s) | Predicted: 2 | Gold: 6\n",
      "2025-07-28 07:50:41,204 - INFO - Run 1 | Q113/515: âŒ WRONG (12.8s) | Predicted: \\frac{4 | Gold: 1.77\n",
      "2025-07-28 07:50:53,651 - INFO - Run 1 | Q114/515: âŒ WRONG (12.4s) | Predicted: 4 | Gold: 6\n",
      "2025-07-28 07:51:04,792 - INFO - Run 1 | Q115/515: âŒ WRONG (11.1s) | Predicted: 1050 | Gold: 900\n",
      "2025-07-28 07:51:13,708 - INFO - Run 1 | Q116/515: âŒ WRONG (8.9s) | Predicted: 209 | Gold: 90.39\n",
      "2025-07-28 07:51:30,627 - INFO - Run 1 | Q117/515: âœ… CORRECT (16.9s) | Predicted: 2 | Gold: 2\n",
      "2025-07-28 07:51:41,480 - INFO - Run 1 | Q118/515: âŒ WRONG (10.9s) | Predicted: ABD | Gold: AD\n",
      "2025-07-28 07:51:46,925 - INFO - Run 1 | Q119/515: âŒ WRONG (5.4s) | Predicted: A | Gold: ACD\n",
      "2025-07-28 07:51:54,399 - INFO - Run 1 | Q120/515: âŒ WRONG (7.5s) | Predicted: 18 | Gold: 7\n",
      "2025-07-28 07:52:04,711 - INFO - Run 1 | Q121/515: âŒ WRONG (10.3s) | Predicted: AD | Gold: BC\n",
      "2025-07-28 07:52:09,023 - INFO - Run 1 | Q122/515: âŒ WRONG (4.3s) | Predicted: 2 | Gold: 4\n",
      "2025-07-28 07:52:18,646 - INFO - Run 1 | Q123/515: âŒ WRONG (9.6s) | Predicted: 1.9 | Gold: 3\n",
      "2025-07-28 07:52:38,729 - INFO - Run 1 | Q124/515: âŒ WRONG (20.1s) | Predicted: 2 | Gold: 1\n",
      "2025-07-28 07:52:48,194 - INFO - Run 1 | Q125/515: âŒ WRONG (9.5s) | Predicted: C | Gold: A\n",
      "2025-07-28 07:52:55,078 - INFO - Run 1 | Q126/515: âœ… CORRECT (6.9s) | Predicted: 0.2 | Gold: 0.2\n",
      "2025-07-28 07:53:03,250 - INFO - Run 1 | Q127/515: âœ… CORRECT (8.2s) | Predicted: A | Gold: A\n",
      "2025-07-28 07:53:18,543 - INFO - Run 1 | Q128/515: âœ… CORRECT (15.3s) | Predicted: 6 | Gold: 6\n",
      "2025-07-28 07:53:27,965 - INFO - Run 1 | Q129/515: âŒ WRONG (9.4s) | Predicted: 715 | Gold: 682\n",
      "2025-07-28 07:53:37,404 - INFO - Run 1 | Q130/515: âŒ WRONG (9.4s) | Predicted: 2 | Gold: 4\n",
      "2025-07-28 07:53:49,996 - INFO - Run 1 | Q131/515: âŒ WRONG (12.6s) | Predicted: 4 | Gold: 6\n",
      "2025-07-28 07:54:02,206 - INFO - Run 1 | Q132/515: âŒ WRONG (12.2s) | Predicted: A | Gold: ABD\n",
      "2025-07-28 07:54:13,682 - INFO - Run 1 | Q133/515: âŒ WRONG (11.5s) | Predicted: 30 | Gold: 25.6\n",
      "2025-07-28 07:54:19,493 - INFO - Run 1 | Q134/515: âœ… CORRECT (5.8s) | Predicted: 6 | Gold: 6\n",
      "2025-07-28 07:54:28,992 - INFO - Run 1 | Q135/515: âœ… CORRECT (9.5s) | Predicted: 0 | Gold: 0\n",
      "2025-07-28 07:54:41,442 - INFO - Run 1 | Q136/515: âŒ WRONG (12.4s) | Predicted: 8 | Gold: 6\n",
      "2025-07-28 07:54:49,009 - INFO - Run 1 | Q137/515: âŒ WRONG (7.6s) | Predicted: 70 | Gold: 495\n",
      "2025-07-28 07:55:04,015 - INFO - Run 1 | Q138/515: âŒ WRONG (15.0s) | Predicted: \\frac{3 | Gold: 6.2\n",
      "2025-07-28 07:55:19,244 - INFO - Run 1 | Q139/515: âŒ WRONG (15.2s) | Predicted: B | Gold: A\n",
      "2025-07-28 07:55:39,801 - INFO - Run 1 | Q140/515: âŒ WRONG (20.6s) | Predicted: 0.954 | Gold: 2\n",
      "2025-07-28 07:55:45,905 - INFO - Run 1 | Q141/515: âŒ WRONG (6.1s) | Predicted: 1 - \\frac{\\sqrt{10 | Gold: 0.83\n",
      "2025-07-28 07:55:55,159 - INFO - Run 1 | Q142/515: âœ… CORRECT (9.3s) | Predicted: A | Gold: A\n",
      "2025-07-28 07:56:08,187 - INFO - Run 1 | Q143/515: âœ… CORRECT (13.0s) | Predicted: CD | Gold: CD\n",
      "2025-07-28 07:56:13,704 - INFO - Run 1 | Q144/515: âŒ WRONG (5.5s) | Predicted: ABC | Gold: ABD\n",
      "2025-07-28 07:56:25,791 - INFO - Run 1 | Q145/515: âŒ WRONG (12.1s) | Predicted: 2.89 \\mu F | Gold: 100.00\n",
      "2025-07-28 07:56:37,216 - INFO - Run 1 | Q146/515: âŒ WRONG (11.4s) | Predicted: 1059 | Gold: 935\n",
      "2025-07-28 07:56:53,685 - INFO - Run 1 | Q147/515: âŒ WRONG (16.5s) | Predicted: ABC | Gold: ACD\n",
      "2025-07-28 07:57:00,438 - INFO - Run 1 | Q148/515: âŒ WRONG (6.8s) | Predicted: ABC | Gold: ACD\n",
      "2025-07-28 07:57:20,083 - INFO - Run 1 | Q149/515: âœ… CORRECT (19.6s) | Predicted: B | Gold: B\n",
      "2025-07-28 07:57:29,970 - INFO - Run 1 | Q150/515: âŒ WRONG (9.9s) | Predicted: C | Gold: D\n",
      "2025-07-28 07:57:29,972 - INFO - ðŸ’¾ State saved: Run 1, 150/5150 questions\n",
      "2025-07-28 07:57:29,973 - INFO - ðŸ“Š Run 1 Progress: 29.1% (150/515 questions)\n",
      "2025-07-28 07:57:35,943 - INFO - Run 1 | Q151/515: âŒ WRONG (6.0s) | Predicted: A | Gold: AC\n",
      "2025-07-28 07:57:43,718 - INFO - Run 1 | Q152/515: âŒ WRONG (7.8s) | Predicted: BC | Gold: ABD\n",
      "2025-07-28 07:57:48,491 - INFO - Run 1 | Q153/515: âŒ WRONG (4.8s) | Predicted: 20 | Gold: 6\n",
      "2025-07-28 07:57:52,330 - INFO - Run 1 | Q154/515: âŒ WRONG (3.8s) | Predicted: A | Gold: B\n",
      "2025-07-28 07:58:03,634 - INFO - Run 1 | Q155/515: âœ… CORRECT (11.3s) | Predicted: AB | Gold: AB\n",
      "2025-07-28 07:58:15,154 - INFO - Run 1 | Q156/515: âŒ WRONG (11.5s) | Predicted: ABCD | Gold: CD\n",
      "2025-07-28 07:58:23,319 - INFO - Run 1 | Q157/515: âŒ WRONG (8.2s) | Predicted: 0 | Gold: 1\n",
      "2025-07-28 07:58:33,410 - INFO - Run 1 | Q158/515: âŒ WRONG (10.1s) | Predicted: \\frac{\\sqrt{15 | Gold: 1.5\n",
      "2025-07-28 07:58:44,357 - INFO - Run 1 | Q159/515: âŒ WRONG (10.9s) | Predicted: B | Gold: ABD\n",
      "2025-07-28 07:59:02,535 - INFO - Run 1 | Q160/515: âœ… CORRECT (18.2s) | Predicted: C | Gold: C\n",
      "2025-07-28 07:59:14,356 - INFO - Run 1 | Q161/515: âŒ WRONG (11.8s) | Predicted: 2 | Gold: 0.95\n",
      "2025-07-28 07:59:35,669 - INFO - Run 1 | Q162/515: âŒ WRONG (21.3s) | Predicted: A | Gold: AD\n",
      "2025-07-28 07:59:46,120 - INFO - Run 1 | Q163/515: âŒ WRONG (10.5s) | Predicted: 0.100 | Gold: 13.32\n",
      "2025-07-28 07:59:52,559 - INFO - Run 1 | Q164/515: âœ… CORRECT (6.4s) | Predicted: B | Gold: B\n",
      "2025-07-28 08:00:03,070 - INFO - Run 1 | Q165/515: âŒ WRONG (10.5s) | Predicted: 2000 | Gold: 569\n",
      "2025-07-28 08:00:16,878 - INFO - Run 1 | Q166/515: âŒ WRONG (13.8s) | Predicted: D | Gold: C\n",
      "2025-07-28 08:00:34,592 - INFO - Run 1 | Q167/515: âœ… CORRECT (17.7s) | Predicted: 4 | Gold: 4\n",
      "2025-07-28 08:00:42,235 - INFO - Run 1 | Q168/515: âŒ WRONG (7.6s) | Predicted: 5 | Gold: 7.5\n",
      "2025-07-28 08:00:56,235 - INFO - Run 1 | Q169/515: âŒ WRONG (14.0s) | Predicted: B | Gold: A\n",
      "2025-07-28 08:01:05,564 - INFO - Run 1 | Q170/515: âŒ WRONG (9.3s) | Predicted: 6.71 | Gold: 130\n",
      "2025-07-28 08:01:16,095 - INFO - Run 1 | Q171/515: âœ… CORRECT (10.5s) | Predicted: 5 | Gold: 5\n",
      "2025-07-28 08:01:23,956 - INFO - Run 1 | Q172/515: âŒ WRONG (7.9s) | Predicted: B | Gold: D\n",
      "2025-07-28 08:01:34,560 - INFO - Run 1 | Q173/515: âœ… CORRECT (10.6s) | Predicted: ABC | Gold: ABC\n",
      "2025-07-28 08:01:43,661 - INFO - Run 1 | Q174/515: âœ… CORRECT (9.1s) | Predicted: C | Gold: C\n",
      "2025-07-28 08:01:53,323 - INFO - Run 1 | Q175/515: âœ… CORRECT (9.7s) | Predicted: 2 | Gold: 2\n",
      "2025-07-28 08:02:05,753 - INFO - Run 1 | Q176/515: âŒ WRONG (12.4s) | Predicted: 894 | Gold: 4\n",
      "2025-07-28 08:02:13,098 - INFO - Run 1 | Q177/515: âŒ WRONG (7.3s) | Predicted: 6.124 | Gold: 6\n",
      "2025-07-28 08:02:26,254 - INFO - Run 1 | Q178/515: âŒ WRONG (13.2s) | Predicted: The ratio \\( \\frac{\\phi_0}{\\phi} \\) cannot be dire | Gold: 6.4\n",
      "2025-07-28 08:02:51,462 - INFO - Run 1 | Q179/515: âŒ WRONG (25.2s) | Predicted: C | Gold: ACD\n",
      "2025-07-28 08:03:08,185 - INFO - Run 1 | Q180/515: âœ… CORRECT (16.7s) | Predicted: 3 | Gold: 3\n",
      "2025-07-28 08:03:15,058 - INFO - Run 1 | Q181/515: âŒ WRONG (6.9s) | Predicted: BC | Gold: C\n",
      "2025-07-28 08:03:30,827 - INFO - Run 1 | Q182/515: âŒ WRONG (15.8s) | Predicted: 0 | Gold: 8\n",
      "2025-07-28 08:03:41,159 - INFO - Run 1 | Q183/515: âŒ WRONG (10.3s) | Predicted: 3 | Gold: 1\n",
      "2025-07-28 08:03:53,904 - INFO - Run 1 | Q184/515: âŒ WRONG (12.7s) | Predicted: \\frac{67 | Gold: 0.8\n",
      "2025-07-28 08:04:06,675 - INFO - Run 1 | Q185/515: âŒ WRONG (12.8s) | Predicted: 3 | Gold: 30\n",
      "2025-07-28 08:04:19,352 - INFO - Run 1 | Q186/515: âŒ WRONG (12.7s) | Predicted: C | Gold: ABD\n",
      "2025-07-28 08:04:27,598 - INFO - Run 1 | Q187/515: âŒ WRONG (8.2s) | Predicted: 1 | Gold: 5\n",
      "2025-07-28 08:04:36,981 - INFO - Run 1 | Q188/515: âœ… CORRECT (9.4s) | Predicted: AB | Gold: AB\n",
      "2025-07-28 08:04:53,043 - INFO - Run 1 | Q189/515: âŒ WRONG (16.1s) | Predicted: 144 | Gold: 108\n",
      "2025-07-28 08:05:36,729 - INFO - Run 1 | Q190/515: âŒ WRONG (43.7s) | Predicted: D | Gold: A\n",
      "2025-07-28 08:05:43,199 - INFO - Run 1 | Q191/515: âœ… CORRECT (6.5s) | Predicted: BC | Gold: BC\n",
      "2025-07-28 08:05:49,480 - INFO - Run 1 | Q192/515: âœ… CORRECT (6.3s) | Predicted: C | Gold: C\n",
      "2025-07-28 08:06:00,744 - INFO - Run 1 | Q193/515: âŒ WRONG (11.3s) | Predicted: 3720 | Gold: 2.32\n",
      "2025-07-28 08:06:12,907 - INFO - Run 1 | Q194/515: âŒ WRONG (12.2s) | Predicted: ACD | Gold: BC\n",
      "2025-07-28 08:06:18,619 - INFO - Run 1 | Q195/515: âœ… CORRECT (5.7s) | Predicted: D | Gold: D\n",
      "2025-07-28 08:06:30,467 - INFO - Run 1 | Q196/515: âŒ WRONG (11.8s) | Predicted: 27 | Gold: 51\n",
      "2025-07-28 08:06:48,788 - INFO - Run 1 | Q197/515: âŒ WRONG (18.3s) | Predicted: -2 | Gold: 4\n",
      "2025-07-28 08:07:02,301 - INFO - Run 1 | Q198/515: âŒ WRONG (13.5s) | Predicted: 0.05 | Gold: 5.56\n",
      "2025-07-28 08:07:09,960 - INFO - Run 1 | Q199/515: âŒ WRONG (7.7s) | Predicted: ACD | Gold: BD\n",
      "2025-07-28 08:07:21,192 - INFO - Run 1 | Q200/515: âœ… CORRECT (11.2s) | Predicted: BC | Gold: BC\n",
      "2025-07-28 08:07:21,193 - INFO - ðŸ’¾ State saved: Run 1, 200/5150 questions\n",
      "2025-07-28 08:07:21,194 - INFO - ðŸ“Š Run 1 Progress: 38.8% (200/515 questions)\n",
      "2025-07-28 08:07:37,132 - INFO - Run 1 | Q201/515: âŒ WRONG (15.9s) | Predicted: AD | Gold: ABD\n",
      "2025-07-28 08:07:52,425 - INFO - Run 1 | Q202/515: âŒ WRONG (15.3s) | Predicted: \\text{less than or equal to | Gold: 1\n",
      "2025-07-28 08:08:04,421 - INFO - Run 1 | Q203/515: âœ… CORRECT (12.0s) | Predicted: 1 | Gold: 1\n",
      "2025-07-28 08:08:19,067 - INFO - Run 1 | Q204/515: âŒ WRONG (14.6s) | Predicted: 4 | Gold: 10\n",
      "2025-07-28 08:08:30,885 - INFO - Run 1 | Q205/515: âŒ WRONG (11.8s) | Predicted: 2998 | Gold: 1219\n",
      "2025-07-28 08:08:47,239 - INFO - Run 1 | Q206/515: âŒ WRONG (16.4s) | Predicted: BD | Gold: BCD\n",
      "2025-07-28 08:08:57,100 - INFO - Run 1 | Q207/515: âŒ WRONG (9.9s) | Predicted: 256 | Gold: 512\n",
      "2025-07-28 08:09:05,831 - INFO - Run 1 | Q208/515: âœ… CORRECT (8.7s) | Predicted: AB | Gold: AB\n",
      "2025-07-28 08:09:16,439 - INFO - Run 1 | Q209/515: âŒ WRONG (10.6s) | Predicted: D | Gold: B\n",
      "2025-07-28 08:09:27,109 - INFO - Run 1 | Q210/515: âŒ WRONG (10.7s) | Predicted: 7 | Gold: 1\n",
      "2025-07-28 08:09:41,919 - INFO - Run 1 | Q211/515: âŒ WRONG (14.8s) | Predicted: 5.538 | Gold: 2\n",
      "2025-07-28 08:09:52,562 - INFO - Run 1 | Q212/515: âŒ WRONG (10.6s) | Predicted: 102 | Gold: 119\n",
      "2025-07-28 08:10:03,200 - INFO - Run 1 | Q213/515: âŒ WRONG (10.6s) | Predicted: C | Gold: A\n",
      "2025-07-28 08:10:12,657 - INFO - Run 1 | Q214/515: âŒ WRONG (9.5s) | Predicted: A | Gold: ABD\n",
      "2025-07-28 08:10:23,111 - INFO - Run 1 | Q215/515: âŒ WRONG (10.5s) | Predicted: A | Gold: B\n",
      "2025-07-28 08:10:36,217 - INFO - Run 1 | Q216/515: âŒ WRONG (13.1s) | Predicted: 20 | Gold: 45\n",
      "2025-07-28 08:10:48,153 - INFO - Run 1 | Q217/515: âŒ WRONG (11.9s) | Predicted: A | Gold: AC\n",
      "2025-07-28 08:10:59,513 - INFO - Run 1 | Q218/515: âŒ WRONG (11.4s) | Predicted: BC | Gold: ABD\n",
      "2025-07-28 08:11:17,526 - INFO - Run 1 | Q219/515: âœ… CORRECT (18.0s) | Predicted: BCD | Gold: BCD\n",
      "2025-07-28 08:11:29,456 - INFO - Run 1 | Q220/515: âœ… CORRECT (11.9s) | Predicted: D | Gold: D\n",
      "2025-07-28 08:11:39,590 - INFO - Run 1 | Q221/515: âŒ WRONG (10.1s) | Predicted: AD | Gold: AC\n",
      "2025-07-28 08:11:45,001 - INFO - Run 1 | Q222/515: âŒ WRONG (5.4s) | Predicted: B | Gold: C\n",
      "2025-07-28 08:11:56,873 - INFO - Run 1 | Q223/515: âŒ WRONG (11.9s) | Predicted: D | Gold: B\n",
      "2025-07-28 08:12:10,873 - INFO - Run 1 | Q224/515: âŒ WRONG (14.0s) | Predicted: 1 | Gold: 6\n",
      "2025-07-28 08:12:20,166 - INFO - Run 1 | Q225/515: âœ… CORRECT (9.3s) | Predicted: BC | Gold: BC\n",
      "2025-07-28 08:12:26,939 - INFO - Run 1 | Q226/515: âœ… CORRECT (6.8s) | Predicted: BCD | Gold: BCD\n",
      "2025-07-28 08:12:41,514 - INFO - Run 1 | Q227/515: âŒ WRONG (14.6s) | Predicted: 0 | Gold: 1\n"
     ]
    }
   ],
   "source": [
    "run_evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adaea0ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
